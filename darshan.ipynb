{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import glob, operator, time, shutil, scipy, sys, os, gzip, subprocess\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "plt.style.use('classic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_by_month(year_month):\n",
    "    year, month = year_month\n",
    "    try:os.mkdir('/projects/AMASE/zliu/facility-data/ds/%d' % year)\n",
    "    except: pass\n",
    "    \n",
    "    try:os.mkdir('/projects/AMASE/zliu/facility-data/ds/%d/%d' % (year, month))\n",
    "    except: pass\n",
    "    \n",
    "    fns = []\n",
    "    for ddir in glob.glob('/gpfs/mira-fs0/logs/darshan/mira/%d/%d/*' % (year, month)):\n",
    "        day = ddir.split('/')[-1]\n",
    "        try: os.mkdir('/projects/AMASE/zliu/facility-data/ds/%d/%d/%s' % (year, month, day))\n",
    "        except: pass\n",
    "        fns += glob.glob('%s/*.gz' % ddir)\n",
    "\n",
    "    for fn in fns[:5]:\n",
    "        with gzip.open(fn, 'rb') as f_in:\n",
    "            ofn = '/projects/AMASE/zliu/facility-data/ds/' + \\\n",
    "                  '/'.join(fn.split('/')[-4:-1]) + '/' + fn.split('/')[-1][:-3]\n",
    "            with open(ofn, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yms = [(2018, m) for m in range(4, 13)] + [(2019, 1), (2019, 2), (2019, 3)]\n",
    "\n",
    "# p = Pool(len(yms))\n",
    "# _ = p.map(parse_by_month, yms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = []\n",
    "year, month = 2018, 8\n",
    "for ddir in glob.glob('/gpfs/mira-fs0/logs/darshan/mira/%d/%d/*' % (year, month)):\n",
    "    fns += glob.glob('%s/*.gz' % ddir)\n",
    "    \n",
    "ds_jids = set()\n",
    "for fn in fns:\n",
    "    _jid = fn.split('/')[-1].split('_')[-3][2:]\n",
    "    ds_jids.add(_jid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_st_jids = set([_jid[:-1] for _jid in open('ap-single-task-jids.txt').readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2252, 2507, 2216, 4872)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ap_st_jids), len(ds_jids), len(ds_jids.intersection(ap_st_jids)), len(fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /projects/AMASE/zliu/facility-data/ds/*.txt ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_by_fn(sfn, option):\n",
    "    ofn = sfn + '.' + option\n",
    "\n",
    "#     os.system('/home/zcliu/usr/bin/darshan-parser --%s %s > %s' % (option, sfn, ofn))\n",
    "#     output = subprocess.getoutput('/home/zcliu/usr/bin/darshan-parser --%s %s' % (option, sfn))\n",
    "    output = os.popen('/home/zcliu/usr/bin/darshan-parser --%s %s' % (option, sfn)).read()\n",
    "    print(output)\n",
    "    \n",
    "fns = []\n",
    "year, month = 2018, 8\n",
    "for ddir in glob.glob('/projects/AMASE/zliu/facility-data/ds/%d/%d/*' % (year, month)):\n",
    "    fns += glob.glob('%s/*.darshan' % ddir)\n",
    "    \n",
    "for fn in fns[:1]:\n",
    "    print (fn)\n",
    "    for opt in ('all', 'base', 'file', 'file-list', 'file-list-detailed', 'perf', 'total')[-2:-1]:\n",
    "        parse_by_fn(fn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_total_from_raw(sfn):\n",
    "    try:\n",
    "        perf = os.popen('/home/zcliu/usr/bin/darshan-parser --total ' + sfn).read()\n",
    "    except:\n",
    "        print(\"error when run darshan-parser for %s\" % sfn)\n",
    "        return None\n",
    "    \n",
    "    posix_wr_time, posix_rd_time, posix_meta_time = None, None, None\n",
    "    mpiio_wr_time, mpiio_rd_time, mpiio_meta_time = None, None, None\n",
    "    for line in perf.split('\\n'):\n",
    "        if line.startswith('total_CP_F_POSIX_READ_TIME:'):\n",
    "            posix_rd_time = float(line.split()[-1])\n",
    "            continue\n",
    "        if line.startswith('total_CP_F_POSIX_WRITE_TIME:'):\n",
    "            posix_wr_time = float(line.split()[-1])\n",
    "            continue\n",
    "        if line.startswith('total_CP_F_POSIX_META_TIME:'):\n",
    "            posix_meta_time = float(line.split()[-1])\n",
    "            continue\n",
    "        if line.startswith('total_CP_F_MPI_READ_TIME:'):\n",
    "            mpiio_rd_time = float(line.split()[-1])\n",
    "            continue\n",
    "        if line.startswith('total_CP_F_MPI_WRITE_TIME:'):\n",
    "            mpiio_wr_time = float(line.split()[-1])\n",
    "            continue\n",
    "        if line.startswith('total_CP_F_MPI_META_TIME:'):\n",
    "            mpiio_meta_time = float(line.split()[-1])\n",
    "            continue\n",
    "    return (posix_wr_time, posix_rd_time, posix_meta_time, \\\n",
    "            mpiio_wr_time, mpiio_rd_time, mpiio_meta_time)\n",
    "\n",
    "    \n",
    "def extract_file_from_raw(sfn):\n",
    "    try:\n",
    "        perf = os.popen('/home/zcliu/usr/bin/darshan-parser --file ' + sfn).read()\n",
    "    except:\n",
    "        print(\"error when run darshan-parser for %s\" % sfn)\n",
    "        return None\n",
    "    \n",
    "    read_bytes, write_bytes, wr_bytes, unique_bytes, shared_bytes = None, None, None, None, None\n",
    "    read_nf, write_nf, wr_nf, unique_nf, shared_nf = None, None, None, None, None\n",
    "    for line in perf.split('\\n'):\n",
    "        if line.startswith('# read_only:'):\n",
    "            lsp = line.split()\n",
    "            read_bytes = int(lsp[-2])\n",
    "            read_nf = int(lsp[-3])\n",
    "            continue\n",
    "        if line.startswith('# write_only:'):\n",
    "            lsp = line.split()\n",
    "            write_bytes = int(lsp[-2])\n",
    "            write_nf = int(lsp[-3])\n",
    "            continue\n",
    "        if line.startswith('# read_write:'):\n",
    "            lsp = line.split()\n",
    "            wr_bytes = int(lsp[-2])\n",
    "            wr_nf = int(lsp[-3])\n",
    "            continue\n",
    "        if line.startswith('# unique:'):\n",
    "            lsp = line.split()\n",
    "            unique_bytes = int(lsp[-2])\n",
    "            unique_nf = int(lsp[-3])\n",
    "            continue\n",
    "        if line.startswith('# shared:'):\n",
    "            lsp = line.split()\n",
    "            shared_bytes = int(lsp[-2])\n",
    "            shared_nf = int(lsp[-3])\n",
    "            continue\n",
    "    return (read_bytes, write_bytes, wr_bytes, unique_bytes, shared_bytes, \\\n",
    "            read_nf, write_nf, wr_nf, unique_nf, shared_nf)\n",
    "    \n",
    "def extract_perf_from_raw(sfn):\n",
    "    try:\n",
    "        perf = os.popen('/home/zcliu/usr/bin/darshan-parser --perf ' + sfn).read()\n",
    "    except:\n",
    "        print(\"error when run darshan-parser for %s\" % sfn)\n",
    "        return None\n",
    "    \n",
    "    total_bytes, slowest_rank_io_time, slowest_rank_meta_time = None, None, None\n",
    "    time_by_cumul_io_only, time_by_cumul_meta_only, time_by_open = None, None, None\n",
    "    agg_perf = None\n",
    "    for line in perf.split('\\n'):\n",
    "        if line.startswith('# total_bytes:'):\n",
    "            total_bytes = int(line.split()[-1])\n",
    "            continue\n",
    "            \n",
    "        if line.startswith('# agg_perf_by_slowest:'):\n",
    "            agg_perf = int(line.split()[-1])\n",
    "            continue\n",
    "            \n",
    "    print(run_time)\n",
    "    \n",
    "def extract_filelist_from_raw(sfn):\n",
    "#     jid = sfn.split('/')[-1].split('_')[-3][2:]\n",
    "    try:\n",
    "        flist = os.popen('/home/zcliu/usr/bin/darshan-parser --file-list ' + sfn).read()\n",
    "    except:\n",
    "        print(\"error when run darshan-parser for %s\" % sfn)\n",
    "        return None\n",
    "    lsp = flist.split('\\n')\n",
    "    jid, idx, cul_time, total_bytes = None, None, 0, 0\n",
    "    start_time, end_time, nprocs = None, None, None\n",
    "    for idx in range(len(lsp)):\n",
    "        if lsp[idx].startswith('# jobid:'): \n",
    "            try: jid = int(lsp[idx].split()[-1])\n",
    "            except: print (\"error when parse: %s\" % lsp[idx])\n",
    "            continue\n",
    "            \n",
    "        if lsp[idx].startswith('# start_time:'): \n",
    "            try: start_time = int(lsp[idx].split()[-1])\n",
    "            except: print (\"error when parse: %s\" % lsp[idx])\n",
    "            continue\n",
    "            \n",
    "        if lsp[idx].startswith('# end_time:'): \n",
    "            try: end_time = int(lsp[idx].split()[-1])\n",
    "            except: print (\"error when parse: %s\" % lsp[idx])\n",
    "            continue\n",
    "            \n",
    "        if lsp[idx].startswith('# nprocs:'): \n",
    "            try: nprocs = int(lsp[idx].split()[-1])\n",
    "            except: print (\"error when parse: %s\" % lsp[idx])\n",
    "            continue\n",
    "            \n",
    "        if lsp[idx].startswith('# <hash>\\t<suffix>'):\n",
    "            break\n",
    "            \n",
    "    iotype2cnt  = {'POSIX':0, 'MPI':0}\n",
    "    iotype2time = {'POSIX':0, 'MPI':0}\n",
    "    for idx in range(idx+1, len(lsp)-1):\n",
    "        _lsp    = lsp[idx].split()\n",
    "        _iotype = _lsp[-4]\n",
    "        try: _iotime = int(_lsp[-3]) * float(_lsp[-1])\n",
    "        except: print (\"error when parse: %s\" % lsp[idx])\n",
    "        cul_time += _iotime\n",
    "        iotype2cnt[_iotype]  += 1\n",
    "        iotype2time[_iotype] += _iotime\n",
    "    ret = (jid, start_time, end_time, nprocs, cul_time, \\\n",
    "           iotype2time.get('POSIX'), iotype2time.get('MPI'), \\\n",
    "           iotype2cnt.get('POSIX'), iotype2cnt.get('MPI'), )\n",
    "    return ret\n",
    "    \n",
    "def extrac_iotime(year_month):\n",
    "    year, month = year_month\n",
    "    fns = []\n",
    "    for ddir in glob.glob('/projects/AMASE/zliu/facility-data/ds/%d/%d/*' % (year, month)):\n",
    "        fns += glob.glob('%s/*.darshan' % ddir)\n",
    "\n",
    "    jid2cul_iotime_val = []\n",
    "    for fn in fns[:]:\n",
    "        _rcd_filelist = extract_filelist_from_raw(fn)\n",
    "        if _rcd_filelist is None: continue\n",
    "        _rcd_file = extract_file_from_raw(fn)\n",
    "        if _rcd_file is None: continue\n",
    "        _rcd_total = extract_total_from_raw(fn)\n",
    "        if _rcd_total is None: continue\n",
    "        \n",
    "        jid2cul_iotime_val.append(_rcd_filelist + _rcd_file + _rcd_total)\n",
    "    jid2cul_iotime = pd.DataFrame(jid2cul_iotime_val, \\\n",
    "                                  columns=('jid', 'start_time', 'end_time', 'nprocs',\\\n",
    "                                           'cul_iotime', 'posix_time', 'mpiio_time', \\\n",
    "                                           'posix_cnt', 'mpiio_cnt', 'read_bytes', \\\n",
    "                                           'write_bytes', 'wr_bytes', 'unique_bytes', \\\n",
    "                                           'shared_bytes', 'read_nf', 'write_nf', \\\n",
    "                                           'wr_nf', 'unique_nf', 'shared_nf', \\\n",
    "                                           'posix_wr_time', 'posix_rd_time', 'posix_meta_time', \\\n",
    "                                           'mpiio_wr_time', 'mpiio_rd_time', 'mpiio_meta_time'))\n",
    "\n",
    "    jid2cul_iotime.to_csv('iotime-%04d%02d.csv' % (year, month), index=False)\n",
    "\n",
    "# yms = [(2018, m) for m in range(2, 13)] + [(2019, 1), (2019, 2), (2019, 3)]\n",
    "# p = Pool(len(yms))\n",
    "# _ = p.map(extrac_iotime, yms)\n",
    "# p.terminate()\n",
    "# p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing 2018-04 @1554911241\n",
      "error when run darshan-parser for /projects/AMASE/zliu/facility-data/ds/2018/4/18/yghadar_lmp_mira_chem_io_id1513985_4-18-80494-5977469701034858038_1.darshan\n",
      "error when run darshan-parser for /projects/AMASE/zliu/facility-data/ds/2018/4/18/yghadar_lmp_mira_chem_io_id1513988_4-18-80831-3690045021201141699_1.darshan\n",
      "2018-04 is done @1554913342\n",
      "start processing 2018-05 @1554913342\n",
      "2018-05 is done @1554913735\n",
      "start processing 2018-06 @1554913735\n",
      "2018-06 is done @1554914086\n",
      "start processing 2018-07 @1554914086\n",
      "2018-07 is done @1554914423\n",
      "start processing 2018-08 @1554914423\n",
      "2018-08 is done @1554914614\n",
      "start processing 2018-09 @1554914614\n",
      "error when run darshan-parser for /projects/AMASE/zliu/facility-data/ds/2018/9/13/33146_nek5000_id1636657_9-13-70112-1348400304312054786_1.darshan\n",
      "2018-09 is done @1554914904\n",
      "start processing 2018-10 @1554914904\n",
      "2018-10 is done @1554915926\n",
      "start processing 2018-11 @1554915926\n",
      "2018-11 is done @1554919794\n",
      "start processing 2018-12 @1554919794\n",
      "2018-12 is done @1554925407\n",
      "start processing 2019-01 @1554925407\n",
      "2019-01 is done @1554945206\n",
      "start processing 2019-02 @1554945206\n",
      "2019-02 is done @1554945364\n",
      "start processing 2019-03 @1554945364\n",
      "2019-03 is done @1554945586\n"
     ]
    }
   ],
   "source": [
    "def extract_all_in_one(fn):\n",
    "    _rcd_filelist = extract_filelist_from_raw(fn)\n",
    "    if _rcd_filelist is None: return None\n",
    "    _rcd_file = extract_file_from_raw(fn)\n",
    "    if _rcd_file is None: return None\n",
    "    _rcd_total = extract_total_from_raw(fn)\n",
    "    if _rcd_total is None: return None\n",
    "    return _rcd_filelist + _rcd_file + _rcd_total\n",
    "\n",
    "def extrac_iotime_parallel(year_month):\n",
    "    year, month = year_month\n",
    "    fns = []\n",
    "    for ddir in glob.glob('/projects/AMASE/zliu/facility-data/ds/%d/%d/*' % (year, month)):\n",
    "        fns += glob.glob('%s/*.darshan' % ddir)\n",
    "\n",
    "    with Pool(256) as p:\n",
    "        jid2cul_iotime_val = p.map(extract_all_in_one, fns)\n",
    "    jid2cul_iotime_val = [v for v in jid2cul_iotime_val if v is not None]\n",
    "    jid2cul_iotime = pd.DataFrame(jid2cul_iotime_val, \\\n",
    "                                  columns=('jid', 'start_time', 'end_time', 'nprocs',\\\n",
    "                                           'cul_iotime', 'posix_time', 'mpiio_time', \\\n",
    "                                           'posix_cnt', 'mpiio_cnt', 'read_bytes', \\\n",
    "                                           'write_bytes', 'wr_bytes', 'unique_bytes', \\\n",
    "                                           'shared_bytes', 'read_nf', 'write_nf', \\\n",
    "                                           'wr_nf', 'unique_nf', 'shared_nf', \\\n",
    "                                           'posix_wr_time', 'posix_rd_time', 'posix_meta_time', \\\n",
    "                                           'mpiio_wr_time', 'mpiio_rd_time', 'mpiio_meta_time'))\n",
    "\n",
    "    jid2cul_iotime.to_csv('iotime-%04d%02d.csv' % (year, month), index=False)\n",
    "    \n",
    "yms = [(2018, m) for m in range(2, 13)] + [(2019, 1), (2019, 2), (2019, 3)]\n",
    "for ym in yms[2:]:\n",
    "    print(\"start processing %d-%02d @%d\" % (ym[0], ym[1], time.time()))\n",
    "    extrac_iotime_parallel(ym)\n",
    "    print(\"%d-%02d is done @%d\" % (ym[0], ym[1], time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387400"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_fns(yms):\n",
    "    fns = []\n",
    "    for year, month in yms:\n",
    "        for ddir in glob.glob('/projects/AMASE/zliu/facility-data/ds/%d/%d/*' % (year, month)):\n",
    "            fns += glob.glob('%s/*.darshan' % ddir)\n",
    "    return fns\n",
    "\n",
    "fns_all = list_fns(yms)\n",
    "len(fns_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projects/AMASE/zliu/facility-data/ds/2018/2/1/bing_IOR_id1399527_2-1-23239-6230366815674728381_1.darshan'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fns_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract_all_from_raw(sfn):\n",
    "    mount_point = {\"/projects\":0, \"/bgsys\":1, \"/\":2, \"/gpfs/mira-fs1\":3, \\\n",
    "                   \"/gpfs/mira-fs0\":4, \"/gpfs/mira-home\":5}\n",
    "    try:\n",
    "        parsed = os.popen('/home/zcliu/usr/bin/darshan-parser --all ' + sfn).read()\n",
    "    except:\n",
    "        print(\"error when run darshan-parser for %s\" % sfn)\n",
    "        return None\n",
    "    ret_frcds = {}\n",
    "    lsp = parsed.split('\\n')\n",
    "    jid, cul_time, total_bytes = None, None, 0\n",
    "    start_time, end_time, nprocs = None, None, None\n",
    "    for idx in range(len(lsp)):\n",
    "        if lsp[idx].startswith('# jobid:'): \n",
    "            jid = int(lsp[idx].split()[-1])\n",
    "            continue\n",
    "            \n",
    "        if lsp[idx].startswith('# start_time:'): \n",
    "            start_time = int(lsp[idx].split()[-1])\n",
    "            continue\n",
    "            \n",
    "        if lsp[idx].startswith('# end_time:'): \n",
    "            end_time = int(lsp[idx].split()[-1])\n",
    "            continue\n",
    "            \n",
    "        if lsp[idx].startswith('# nprocs:'): \n",
    "            nprocs = int(lsp[idx].split()[-1])\n",
    "            continue\n",
    "            \n",
    "        if lsp[idx].startswith('#<rank>'):\n",
    "            break\n",
    "            \n",
    "#     if not lsp[idx].startswith('#<rank>'): return None\n",
    "    global_idx = idx + 1\n",
    "    while True:\n",
    "        f_counter = []\n",
    "        for idx in range(global_idx, global_idx+162):\n",
    "            _lsp = lsp[idx].split()\n",
    "            if len(f_counter)==0:\n",
    "                f_counter = [_lsp[1], _lsp[0], mount_point[_lsp[-2]]]\n",
    "            f_counter.append(_lsp[3])\n",
    "\n",
    "        ret_frcds[f_counter[0]] = tuple(f_counter[1:])\n",
    "        global_idx = idx + 1\n",
    "        if lsp[global_idx].startswith('total_CP_'): break\n",
    "        \n",
    "    for idx in range(global_idx, len(lsp)):\n",
    "        if lsp[idx].startswith('# <hash>\\t<suffix>'): break\n",
    "            \n",
    "    global_idx = idx + 1\n",
    "    f_cnt = 0\n",
    "    for idx in range(global_idx, len(lsp)):\n",
    "        _lsp    = lsp[idx].split()\n",
    "        ret_frcds[_lsp[0]] += tuple(_lsp[2:6])\n",
    "        f_cnt += 1\n",
    "        if f_cnt == len(ret_frcds): break\n",
    "    \n",
    "    return [(f, jid)+ret_frcds[f] for f in ret_frcds.keys()]\n",
    "\n",
    "# val_tbl = []\n",
    "# for fn in list_fns(((2018, 3), )):\n",
    "#     _ = extract_all_from_raw(fn)\n",
    "#     if _ is not None: val_tbl.append(_)\n",
    "\n",
    "for ym in [(2018, m) for m in range(2, 13)] + [(2019, 1), (2019, 2), (2019, 3)]:\n",
    "    val_tbl = []\n",
    "    fns = list_fns((ym, ))\n",
    "    with Pool(128) as p:\n",
    "        val_tbl_bytask = p.map(extract_all_from_raw, fns[:])\n",
    "\n",
    "    for vtbl in val_tbl_bytask:\n",
    "        if vtbl is not None:\n",
    "            val_tbl += vtbl\n",
    "    val_tbl = np.array(val_tbl)\n",
    "\n",
    "    cols_opt = pd.read_csv('f_counter_name.txt')\n",
    "    f_counter_mask = cols_opt.opt_in.values==1\n",
    "    f_counters = cols_opt.counters.values[f_counter_mask]\n",
    "\n",
    "    pd.DataFrame(val_tbl, columns=f_counters).to_csv('file-records-%d%02d.csv' % (ym[0], ym[1]), \\\n",
    "                                                                        index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.popen('/home/zcliu/usr/bin/darshan-parser --all ' + fns[0]).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CP_F_VARIANCE_RANK_BYTES', '']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('f_counter_name.txt').read().split('\\n')[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "jid2cul_iotime.to_csv('iotime-201808.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = []\n",
    "for ddir in glob.glob('/projects/AMASE/zliu/facility-data/ds/%d/%d/*' % (2018, 5))[:1]:\n",
    "    fns += glob.glob('%s/*.darshan' % ddir)\n",
    "\n",
    "for fn in fns[:1]:\n",
    "    _rcd_filelist = extract_filelist_from_raw(fn)\n",
    "    if _rcd_filelist is None: continue\n",
    "    _rcd_file = extract_file_from_raw(fn)\n",
    "    if _rcd_file is None: continue\n",
    "    _rcd_total = extract_total_from_raw(fn)\n",
    "    if _rcd_total is None: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = list_fns(((2018, 5), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1524752, 1525133257, 1525133263, 1): ([], [982515712, 110])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_filesize_info(sfn):\n",
    "    mount_point = {\"/projects\":0, \"/bgsys\":1, \"/\":2, \"/gpfs/mira-fs1\":3, \\\n",
    "                   \"/gpfs/mira-fs0\":4, \"/gpfs/mira-home\":5}\n",
    "    try:\n",
    "        parsed_fd = os.popen('/home/zcliu/usr/bin/darshan-parser --all ' + sfn)#.read()\n",
    "    except:\n",
    "        print(\"error when run darshan-parser for %s\" % sfn)\n",
    "        return None\n",
    "\n",
    "    jid, start_time, end_time, nprocs = None, None, None, None\n",
    "    for line in parsed_fd:\n",
    "        \n",
    "        if line.startswith('# jobid:'): \n",
    "            jid = int(line.split()[-1])\n",
    "            continue\n",
    "            \n",
    "        if line.startswith('# start_time:'): \n",
    "            start_time = int(line.split()[-1])\n",
    "            continue\n",
    "            \n",
    "        if line.startswith('# end_time:'): \n",
    "            end_time = int(line.split()[-1])\n",
    "            continue\n",
    "            \n",
    "        if line.startswith('# nprocs:'): \n",
    "            nprocs = int(line.split()[-1])\n",
    "            continue\n",
    "            \n",
    "        if line.startswith('#<rank>'):\n",
    "            break\n",
    "    \n",
    "    task2frcd, file_rcd = {}, []\n",
    "    task_key = (jid, start_time, end_time, nprocs)\n",
    "    for line in parsed_fd:\n",
    "        if line.startswith('total_CP_'): break\n",
    "        file_rcd.append(line)# += line        \n",
    "    file_rcd = pd.DataFrame([x.split() for x in file_rcd], \\\n",
    "          columns=('rank', 'filename', 'counter', 'value', 'fn_suffix', 'mount_pt', 'fs_type'))\n",
    "    \n",
    "    read_rcd  = file_rcd[(file_rcd.counter=='CP_BYTES_READ') & (file_rcd.value>'0')]\n",
    "    write_rcd = file_rcd[(file_rcd.counter=='CP_BYTES_WRITTEN') & (file_rcd.value>'0')]\n",
    "    _rrcd = [int(x) for x in read_rcd.value.values]\n",
    "    _wrcd = [int(x) for x in write_rcd.value.values]\n",
    "    \n",
    "    task2frcd[task_key] = (_rrcd, _wrcd)\n",
    "    return task2frcd\n",
    "    \n",
    "extract_filesize_info(fns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ym in [(2018, m) for m in range(2, 13)] + [(2019, 1), (2019, 2), (2019, 3)]:\n",
    "    val_tbl = []\n",
    "    fns = list_fns((ym, ))\n",
    "    with Pool(128) as p:\n",
    "        val_tbl_bytask = p.map(extract_filesize_info, fns[:])\n",
    "    cmb_res = {}\n",
    "    for vtbl in val_tbl_bytask:\n",
    "        if vtbl is not None: \n",
    "            cmb_res = {**vtbl, **cmb_res}\n",
    "\n",
    "    np.save(\"fsize-records-%d-%02d\" % (ym[0], ym[1]), cmb_res)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcd = np.load(\"fsize-records-2018-02.npy\")[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(1398441, 1517445391, 1517445412, 4096), (1396772, 1517493356, 1517500553, 512), (1399527, 1517466439, 1517466526, 800), (1400043, 1517468671, 1517468688, 800), (1399816, 1517468671, 1517468678, 2048), (1399527, 1517467218, 1517467232, 3200), (1396772, 1517493428, 1517500610, 512), (1396772, 1517493393, 1517500557, 512), (1402385, 1517504910, 1517504924, 1024), (1399527, 1517467383, 1517467396, 3200)])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
