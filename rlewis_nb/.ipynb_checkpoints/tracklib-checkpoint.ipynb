{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luckierdodge/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import datetime\n",
    "import heapq as heap\n",
    "from os.path import basename\n",
    "# Imports to use TensorFlow\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# Imports to graph t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "# Imports for xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Imports for Elapsed Time Prediction\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Pandas options\n",
    "pd.options.display.max_columns = 1200\n",
    "pd.options.display.max_rows = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Combine Tracklib and Cobalt DJC logs\n",
    "\n",
    "If you have already run Section 1, or have an existing copy of the combined `.csv`, skip to **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in TrackLib dataset\n",
    "libdf = pd.read_csv('../data/tracklib/tracklib_mira_20200227.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Cobalt logs dataset\n",
    "djc_df = pd.read_csv('../data/tracklib/mira_djc_complete/dim_job_composite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COBALT_JOBID</th>\n",
       "      <th>QUEUED_DATE_ID</th>\n",
       "      <th>START_DATE_ID</th>\n",
       "      <th>END_DATE_ID</th>\n",
       "      <th>WALLTIME_SECONDS</th>\n",
       "      <th>RUNTIME_SECONDS</th>\n",
       "      <th>NODES_USED</th>\n",
       "      <th>NODES_REQUESTED</th>\n",
       "      <th>CORES_USED</th>\n",
       "      <th>CORES_REQUESTED</th>\n",
       "      <th>...</th>\n",
       "      <th>gsl</th>\n",
       "      <th>netcdf</th>\n",
       "      <th>valgrind</th>\n",
       "      <th>essl_fftw</th>\n",
       "      <th>silo</th>\n",
       "      <th>bgclang</th>\n",
       "      <th>essl</th>\n",
       "      <th>darshan</th>\n",
       "      <th>scorep</th>\n",
       "      <th>metis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.763140e+05</td>\n",
       "      <td>2.763140e+05</td>\n",
       "      <td>2.763140e+05</td>\n",
       "      <td>2.763140e+05</td>\n",
       "      <td>276314.000000</td>\n",
       "      <td>276314.000000</td>\n",
       "      <td>276314.000000</td>\n",
       "      <td>276314.000000</td>\n",
       "      <td>276314.000000</td>\n",
       "      <td>276314.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.046564e+06</td>\n",
       "      <td>2.016660e+07</td>\n",
       "      <td>2.016663e+07</td>\n",
       "      <td>2.016663e+07</td>\n",
       "      <td>11508.103838</td>\n",
       "      <td>7349.374773</td>\n",
       "      <td>2053.188300</td>\n",
       "      <td>2013.286880</td>\n",
       "      <td>32851.012804</td>\n",
       "      <td>32212.590082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.344312e+05</td>\n",
       "      <td>1.365261e+04</td>\n",
       "      <td>1.364564e+04</td>\n",
       "      <td>1.364466e+04</td>\n",
       "      <td>14334.547058</td>\n",
       "      <td>11750.518818</td>\n",
       "      <td>4310.332276</td>\n",
       "      <td>4307.036097</td>\n",
       "      <td>68965.316411</td>\n",
       "      <td>68912.577552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.522200e+05</td>\n",
       "      <td>2.014103e+07</td>\n",
       "      <td>2.014123e+07</td>\n",
       "      <td>2.015010e+07</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.183280e+05</td>\n",
       "      <td>2.015093e+07</td>\n",
       "      <td>2.015093e+07</td>\n",
       "      <td>2.015093e+07</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>425.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>8192.000000</td>\n",
       "      <td>8192.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.727955e+05</td>\n",
       "      <td>2.016120e+07</td>\n",
       "      <td>2.016121e+07</td>\n",
       "      <td>2.016121e+07</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>2666.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>8192.000000</td>\n",
       "      <td>8192.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.538465e+06</td>\n",
       "      <td>2.018051e+07</td>\n",
       "      <td>2.018051e+07</td>\n",
       "      <td>2.018051e+07</td>\n",
       "      <td>18000.000000</td>\n",
       "      <td>9563.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>32768.000000</td>\n",
       "      <td>32768.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.999000e+07</td>\n",
       "      <td>2.019073e+07</td>\n",
       "      <td>2.019073e+07</td>\n",
       "      <td>2.019073e+07</td>\n",
       "      <td>217200.000000</td>\n",
       "      <td>217243.000000</td>\n",
       "      <td>49152.000000</td>\n",
       "      <td>49152.000000</td>\n",
       "      <td>786432.000000</td>\n",
       "      <td>786432.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       COBALT_JOBID  QUEUED_DATE_ID  START_DATE_ID   END_DATE_ID  \\\n",
       "count  2.763140e+05    2.763140e+05   2.763140e+05  2.763140e+05   \n",
       "mean   1.046564e+06    2.016660e+07   2.016663e+07  2.016663e+07   \n",
       "std    5.344312e+05    1.365261e+04   1.364564e+04  1.364466e+04   \n",
       "min    3.522200e+05    2.014103e+07   2.014123e+07  2.015010e+07   \n",
       "25%    6.183280e+05    2.015093e+07   2.015093e+07  2.015093e+07   \n",
       "50%    9.727955e+05    2.016120e+07   2.016121e+07  2.016121e+07   \n",
       "75%    1.538465e+06    2.018051e+07   2.018051e+07  2.018051e+07   \n",
       "max    9.999000e+07    2.019073e+07   2.019073e+07  2.019073e+07   \n",
       "\n",
       "       WALLTIME_SECONDS  RUNTIME_SECONDS     NODES_USED  NODES_REQUESTED  \\\n",
       "count     276314.000000    276314.000000  276314.000000    276314.000000   \n",
       "mean       11508.103838      7349.374773    2053.188300      2013.286880   \n",
       "std        14334.547058     11750.518818    4310.332276      4307.036097   \n",
       "min          180.000000        15.000000       0.000000         1.000000   \n",
       "25%         2400.000000       425.000000     512.000000       512.000000   \n",
       "50%         6000.000000      2666.000000     512.000000       512.000000   \n",
       "75%        18000.000000      9563.000000    2048.000000      2048.000000   \n",
       "max       217200.000000    217243.000000   49152.000000     49152.000000   \n",
       "\n",
       "          CORES_USED  CORES_REQUESTED  ...       gsl    netcdf  valgrind  \\\n",
       "count  276314.000000    276314.000000  ...  276314.0  276314.0  276314.0   \n",
       "mean    32851.012804     32212.590082  ...       0.0       0.0       0.0   \n",
       "std     68965.316411     68912.577552  ...       0.0       0.0       0.0   \n",
       "min         0.000000        16.000000  ...       0.0       0.0       0.0   \n",
       "25%      8192.000000      8192.000000  ...       0.0       0.0       0.0   \n",
       "50%      8192.000000      8192.000000  ...       0.0       0.0       0.0   \n",
       "75%     32768.000000     32768.000000  ...       0.0       0.0       0.0   \n",
       "max    786432.000000    786432.000000  ...       0.0       0.0       0.0   \n",
       "\n",
       "       essl_fftw      silo   bgclang      essl   darshan    scorep     metis  \n",
       "count   276314.0  276314.0  276314.0  276314.0  276314.0  276314.0  276314.0  \n",
       "mean         0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "std          0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "min          0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "25%          0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "50%          0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "75%          0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "max          0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[8 rows x 80 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take cobalt logs and add a column for each short library name\n",
    "df = djc_df.assign(**dict.fromkeys(libdf.LIB_SHORT_NAME.unique(), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode library names into djc_df\n",
    "# THIS IS VERY INEFFICIENT, DON'T RUN UNLESS YOU NEED TO.\n",
    "# TAKES ~1 hour to run on flick ðŸ™ƒ\n",
    "for lib in libdf.LIB_SHORT_NAME.unique():\n",
    "    temp_df = libdf[libdf['LIB_SHORT_NAME'] == lib]\n",
    "    temp_df.set_index('COBALT_JOBID', inplace=True)\n",
    "    def helper(row):\n",
    "        if row['COBALT_JOBID'] in temp_df.index:\n",
    "            row[lib] = 1\n",
    "            return row\n",
    "        else:\n",
    "            return row\n",
    "    df = df.apply(helper, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write it out to .csv so we don't have to run this terribly inefficient\n",
    "# code every time\n",
    "df.to_csv('../data/tracklib/tracklib_djc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Clean and Filter Combined Dataset\n",
    "\n",
    "If you already have a filtered `.csv` you want to use, skip to **Section 3**.\n",
    "\n",
    "* Parameters:\n",
    "    * n\n",
    "        * Number of top corehours consumers to include\n",
    "        * Possible values: >=0\n",
    "        * Set to 0 to include all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the various dataframes if you need to\n",
    "df = pd.read_csv('../data/tracklib/tracklib_djc.csv', header=0, low_memory=False)\n",
    "libdf = pd.read_csv('../data/tracklib/tracklib_mira_20200227.csv')\n",
    "djc_df = pd.read_csv('../data/mira_djc_complete/dim_job_composite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Nans and weird column \n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter combined dataset based on date overlap\n",
    "df = df[df.START_TIMESTAMP > libdf.START_TIMESTAMP.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for an executable name/project name feature\n",
    "def exec_maker(row):\n",
    "    return str('%s.%s' % (row['PROJECT_NAME'], basename(row['COMMAND']))) \n",
    "\n",
    "df['EXEC'] = df.apply(exec_maker, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by top n core hour consumers\n",
    "# df_filtered = pd.DataFrame()\n",
    "# if n > 0:\n",
    "#     names = df.EXEC.unique()\n",
    "#     core_hours_dict = {}\n",
    "#     for name in names:\n",
    "#         core_hours_dict.update({name : df.USED_CORE_HOURS.sum()})\n",
    "#     top_names = heap.nlargest(n, core_hours_dict, key=core_hours_dict.get)\n",
    "#     for name in top_names:\n",
    "#         df_filtered = df_filtered.append(df[df['EXEC'] == name])\n",
    "# else:\n",
    "#     df_filtered = df.copy()\n",
    "\n",
    "n_top_execs = df.EXEC.value_counts().head(n - 1)\n",
    "df_filtered = df[df.EXEC.isin(n_top_execs.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a '.csv'\n",
    "if n > 0:\n",
    "    df_filtered.to_csv('../data/tracklib/tracklib_djc_filtered_' + str(n) + '.csv')\n",
    "else:\n",
    "    df_filtered.to_csv('../data/tracklib/tracklib_djc_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: TensorFlow Model\n",
    "\n",
    "* Parameters:\n",
    "    * m\n",
    "        * number of top corehours consumers to include.\n",
    "        * Possible values: >=0\n",
    "        * Set to 0 to include all of them. \n",
    "        * Set to n to use the value from **Section 2**.\n",
    "    * train_size\n",
    "        * how much of the full dataset to use for training\n",
    "        * Possible values: 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "m = 25\n",
    "train_size = .80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the various dataframes if you need to\n",
    "# df = pd.read_csv('../data/tracklib/tracklib_djc.csv')\n",
    "# libdf = pd.read_csv('../data/tracklib/tracklib_mira_20200227.csv')\n",
    "# djc_df = pd.read_csv('../data/mira_djc_complete/dim_job_composite.csv')\n",
    "if m > 0:\n",
    "    df_filtered = pd.read_csv('../data/tracklib/tracklib_djc_filtered_' + str(m) + '.csv')\n",
    "else:\n",
    "    df_filtered = pd.read_csv('../data/tracklib/tracklib_djc_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and targets to use for model\n",
    "features = df_filtered.select_dtypes(include=[np.number]).copy()\n",
    "features = features.drop([column for column in features.columns if features[column].max() == features[column].min()], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'ID' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'unnamed' in column.lower()], axis=1)\n",
    "\n",
    "targets = pd.DataFrame()\n",
    "targets['number'] = pd.factorize(df_filtered['EXEC'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXPERIMENT: Drop everthing except libraries\n",
    "# features = features.drop([column for column in features.columns if 'NUM' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'SECONDS' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'USED' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'CORE' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'NODE' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'IS' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'EXIT' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'FACTOR' in column], axis=1)\n",
    "\n",
    "# for columns in features.columns:\n",
    "#     print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXPERIMENT: Test without libraries\n",
    "# features = features.drop([column for column in features.columns if column.islower()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WALLTIME_SECONDS', 'NODES_REQUESTED', 'CORES_REQUESTED',\n",
       "       'REQUESTED_CORE_HOURS', 'OVERBURN_CORE_HOURS', 'bgsys-gnu-linux',\n",
       "       'bgsys-gnu-linux-4.7.2', 'autoperf', 'bgsys-comm', 'bgsys-hwi',\n",
       "       'ibmcmp', 'bgsys-bgpm', 'bgsys-spi', 'qmcpack', 'bgsys-cnk', 'fftw',\n",
       "       'zlib', 'hdf5', 'lapack', 'bgclang', 'essl', 'darshan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXPERIMENT: Drop features we don't have access to before runtime\n",
    "features = features.drop([column for column in features.columns if 'RUNTIME_SECONDS' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'NODES_USED' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'CORES_USED' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'EXIT_STATUS' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'USED_CORE_HOURS' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'COBALT_NUM_TASKS' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'ELIGIBLE' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'QUEUED' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'USAGE' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'EXIT_CODE' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'NUM_TASK' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'IS_' in column], axis=1)\n",
    "\n",
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features between 0 and 1 - plays nice with our neural network\n",
    "# features = ((features - features.mean()) / (features.max() - features.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate our data into training and test sets\n",
    "train_examples = features.head(int(len(df_filtered.index) * train_size))\n",
    "train_targets = targets.head(int(len(df_filtered.index) * train_size))\n",
    "test_examples = features.tail(int(len(df_filtered.index) * (1-train_size)))\n",
    "test_targets = targets.tail(int(len(df_filtered.index) * (1-train_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luckierdodge/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Define our TensorFlow model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(256, activation='relu', input_shape=(len(features.columns.values),)),\n",
    "  tf.keras.layers.Dense(128, activation='relu', input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(128, activation='relu', input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(128, activation='relu', input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(targets.number.unique().size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile our model\n",
    "model.compile(optimizer=tf.train.AdagradOptimizer(0.05),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luckierdodge/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 47044 samples\n",
      "Epoch 1/10\n",
      "47044/47044 [==============================] - 3s 56us/sample - loss: 239.5653 - sparse_categorical_accuracy: 0.1297\n",
      "Epoch 2/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.9880 - sparse_categorical_accuracy: 0.1455\n",
      "Epoch 3/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.7578 - sparse_categorical_accuracy: 0.2025\n",
      "Epoch 4/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.6550 - sparse_categorical_accuracy: 0.2187\n",
      "Epoch 5/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.6119 - sparse_categorical_accuracy: 0.2265\n",
      "Epoch 6/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.6083 - sparse_categorical_accuracy: 0.2309\n",
      "Epoch 7/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.5885 - sparse_categorical_accuracy: 0.2367\n",
      "Epoch 8/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.5647 - sparse_categorical_accuracy: 0.2412\n",
      "Epoch 9/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.5454 - sparse_categorical_accuracy: 0.2433\n",
      "Epoch 10/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.5231 - sparse_categorical_accuracy: 0.2452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f21906fbf50>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(train_examples,\n",
    "          train_targets,\n",
    "          epochs=10,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11760/11760 [==============================] - 0s 21us/sample - loss: 2.5017 - sparse_categorical_accuracy: 0.2456\n",
      "Test accuracy: [2.5017234156731845, 0.24557823]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Test Accuracy to make sure we haven't overfit\n",
    "test_acc = model.evaluate(test_examples, test_targets)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: T-SNE Model\n",
    "\n",
    "Use Zhengchung's t-sne plotting code to make a half-decent plot of a T-SNE representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the various dataframes if you need to\n",
    "# df = pd.read_csv('../data/tracklib/tracklib_djc.csv')\n",
    "# libdf = pd.read_csv('../data/tracklib/tracklib_mira_20200227.csv')\n",
    "# djc_df = pd.read_csv('../data/mira_djc_complete/dim_job_composite.csv')\n",
    "if m > 0:\n",
    "    df_filtered = pd.read_csv('../data/tracklib/tracklib_djc_filtered_' + str(m) + '.csv')\n",
    "else:\n",
    "    df_filtered = pd.read_csv('../data/tracklib/tracklib_djc_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_filtered.select_dtypes(include=[np.number]).copy()\n",
    "features = features.drop([column for column in features.columns if features[column].max() == features[column].min()], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'ID' in column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-e3c8c978022f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train T-SNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_embedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \"\"\"\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    796\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     def _tsne(self, P, degrees_of_freedom, n_samples, X_embedded,\n",
      "\u001b[0;32m~/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_iter_without_progress'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_without_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m             params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0;32m--> 852\u001b[0;31m                                                           **opt_args)\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;31m# Save the final number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compute_error'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_convergence\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[1;32m    260\u001b[0m                                       \u001b[0mdof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                                       \u001b[0mcompute_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                                       num_threads=num_threads)\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train T-SNE\n",
    "X_embedded = TSNE().fit_transform(features)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_sne_vis_by_group(x_emb, exe_idn, topn=10):\n",
    "    idn_unique, idn_count = np.unique(exe_idn, return_counts=True)\n",
    "    top_idn = idn_unique[np.argsort(idn_count)[-topn:]]\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    colors = ('g', 'b', 'gold', 'yellow', 'tan', 'cyan', 'magenta', 'black', 'orange', 'darkgreen')\n",
    "    markers= ('x', 'o', '>', '<', 's', 'v', 'H', 'D', '3', '1', '2')\n",
    "    _other_grp = np.zeros(exe_idn.shape[0], dtype=np.bool)\n",
    "    for _idx, _idn in enumerate(top_idn):\n",
    "        _emb_grp = x_emb[exe_idn == _idn]\n",
    "        plt.plot(_emb_grp[:, 0], _emb_grp[:, 1], markers[_idx], alpha=.8, color=colors[_idx], markersize=6, label = _idn)\n",
    "        _other_grp |= (exe_idn == _idn)\n",
    "#         print(\"%s is marked by %s and %s\" % (_idn, colors[_idx], markers[_idx]))\n",
    "    _uncat = x_emb[~_other_grp]\n",
    "    plt.plot(_uncat[:, 0], _uncat[:, 1], markers[-1], alpha=.8, color='r', markersize=6, label = 'Others')\n",
    "    \n",
    "    plt.xlim(left=X_embedded[:, 0].min()*1.05, right=X_embedded[:, 0].max()*1.05)\n",
    "    plt.ylim(bottom=X_embedded[:, 1].min()*1.05, top=X_embedded[:, 1].max()*1.05)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.legend(bbox_to_anchor=(0., 1.0, 1., .102), ncol=4, loc=3, fancybox=True, framealpha=0.5, fontsize=14)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "t_sne_vis_by_group(X_embedded, df_filtered['EXEC'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "m = 25\n",
    "train_size = .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the various dataframes if you need to\n",
    "# df = pd.read_csv('../data/tracklib/tracklib_djc.csv')\n",
    "# libdf = pd.read_csv('../data/tracklib/tracklib_mira_20200227.csv')\n",
    "# djc_df = pd.read_csv('../data/mira_djc_complete/dim_job_composite.csv')\n",
    "if m > 0:\n",
    "    df_filtered = pd.read_csv('../data/tracklib/tracklib_djc_filtered_' + str(m) + '.csv')\n",
    "else:\n",
    "    df_filtered = pd.read_csv('../data/tracklib/tracklib_djc_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[df_filtered.WALLTIME_SECONDS >= df_filtered.RUNTIME_SECONDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[df_filtered['EXIT_STATUS'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42678"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_filtered.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "features['WALLTIME_SECONDS'] = df_filtered['WALLTIME_SECONDS']\n",
    "for column in df_filtered.columns:\n",
    "    if column.islower():\n",
    "        features[column] = df_filtered[column]\n",
    "\n",
    "cols = ['USERNAME', 'PROJECT_NAME', 'QUEUE_NAME', 'EXEC', 'NODES_REQUESTED', 'REQUESTED_CORE_HOURS']\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False, categories='auto')\n",
    "enc.fit(df_filtered[cols])\n",
    "features = pd.concat([features, pd.DataFrame(enc.transform(df_filtered[cols])).reindex(features.index)], axis=1, join='inner')\n",
    "\n",
    "targets = df_filtered.RUNTIME_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and targets to use for model\n",
    "# features = df_filtered.select_dtypes(include=[np.number]).copy()\n",
    "# features = features.drop([column for column in features.columns if features[column].max() == features[column].min()], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'ID' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'unnamed' in column.lower()], axis=1)\n",
    "\n",
    "# targets = df_filtered.RUNTIME_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXPERIMENT: Test without libraries\n",
    "# features = features.drop([column for column in features.columns if str(column).islower()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = LabelBinarizer()\n",
    "# enc.fit(df_filtered.EXEC.values.reshape(-1, 1))\n",
    "# encoding = enc.transform(df_filtered.EXEC.values.reshape(-1,1))\n",
    "# features = pd.concat([features, pd.DataFrame(encoding)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = features.head(int(len(df_filtered.index) * train_size))\n",
    "train_targets = targets.head(int(len(df_filtered.index) * train_size))\n",
    "test_examples = features.tail(int(len(df_filtered.index) * (1-train_size)))\n",
    "test_targets = targets.tail(int(len(df_filtered.index) * (1-train_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", max_depth=10)\n",
    "xgb_model.fit(train_examples, train_targets.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_targets, xgb_model.predict(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elapsed Time Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'n_estimators'    :[100, 200, 300, 400, 500, 600, 1000][-3:],\\\n",
    "#          'max_depth'       :[5, 10, 20, 30, 40, 50][-3:],} \n",
    "param_grid = {\n",
    "    'n_estimators': [400, 700, 1000],\n",
    "    'colsample_bytree': [0.7, 0.8],\n",
    "    'max_depth': [15,20,25],\n",
    "    'reg_alpha': [1.1, 1.2, 1.3],\n",
    "    'reg_lambda': [1.1, 1.2, 1.3],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'objective': \"multi:softprob\"\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "grid = GridSearchCV(estimator = xgb.XGBRegressor(), param_grib = param_grid, n_jobs=-1, cv=5, scoring_fit='neg_mean_squared_error')\n",
    "\n",
    "grid.fit(train_examples, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred    = grid.best_estimator_.predict(test_examples)\n",
    "error   = pred - test_targets\n",
    "abs_err = np.abs(error)\n",
    "rel_err = 100. * abs_err / test_targets\n",
    "print(np.percentile(rel_err, 25), np.percentile(rel_err, 50), np.percentile(rel_err, 75), np.percentile(rel_err, 95))\n",
    "print(np.percentile(abs_err, 25), np.percentile(abs_err, 50), np.percentile(abs_err, 75), np.percentile(abs_err, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rel_err, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feat_idx = np.argsort(grid.best_estimator_.feature_importances_)[-15:]\n",
    "features.columns[top_feat_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(pred)):\n",
    "#     pred[i] = min(pred[i], test_examples.WALLTIME_SECONDS.to_list()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = df_filtered.NODES_REQUESTED.tail(int(len(df_filtered.index) * (1-train_size)))\n",
    "cushion = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of difference between runtime and new walltime w/ cushion\n",
    "((pred + cushion) - test_targets).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of jobs cutoff w/ new walltimes\n",
    "array = ((pred + cushion) - test_targets)\n",
    "print(len([item for item in array if item <= 0]))\n",
    "[item for item in array if item <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i in range(len(array.values)) if array.values[i] <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = test_examples.iloc[indices]\n",
    "examples = examples.drop(examples.columns[(examples == 0).all()], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walltimes = test_targets.iloc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.loc[4955]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of jobs cutoff\n",
    "len([item for item in ((pred + cushion) - test_targets) if item <= 0])/len(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walltime hours cutoff\n",
    "sum([item for item in (((pred + cushion) - test_targets)) if item <= 0]) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodehours cutoff\n",
    "sum([item for item in (((pred + cushion) - test_targets) * nodes) if item <= 0]) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodehours saved\n",
    "((test_examples.WALLTIME_SECONDS - (pred + cushion)) * nodes).sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total nodehours\n",
    "(test_examples.WALLTIME_SECONDS * nodes).sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved nodehours percentage\n",
    "((test_examples.WALLTIME_SECONDS - (pred + cushion)) * nodes).sum() / (test_examples.WALLTIME_SECONDS * nodes).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total walltime cutoff\n",
    "sum([item for item in (((pred + cushion) - test_targets)) if item <= 0]) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total walltime saved\n",
    "((test_examples.WALLTIME_SECONDS - (pred + cushion))).sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total walltime hours scheduled\n",
    "(test_examples.WALLTIME_SECONDS).sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved walltime percentage\n",
    "((test_examples.WALLTIME_SECONDS - (pred + cushion))).sum() / (test_examples.WALLTIME_SECONDS * nodes).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of difference between old walltime and new walltime w/ cushion\n",
    "((test_examples.WALLTIME_SECONDS - (pred + cushion))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of difference between old walltime and new walltime w/ cushion in nodehours\n",
    "((test_examples.WALLTIME_SECONDS - (pred + cushion)) * nodes).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
