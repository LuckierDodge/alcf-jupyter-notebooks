{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import os\n",
    "import time\n",
    "pd.options.display.max_columns = 1200\n",
    "pd.options.display.max_rows = 1200\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/autoperf_filtered_1000000ch', header=0).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['zero_execName'] == 'nek5000'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train/Test and Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['zero_stdMpiIoTime']\n",
    "features = df.columns.tolist()\n",
    "remove = ['zero_execName', 'run_date', 'zero_userName', 'zero_threadMode', 'min_userName', 'min_execName', 'min_threadMode', 'max_execName', 'max_userName', 'max_threadMode', 'av_execName', 'av_userName', 'av_threadMode']\n",
    "for re in remove:\n",
    "    features.remove(re)\n",
    "df = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = .8\n",
    "train = df.head(int(df.size * train_size))\n",
    "test = df.tail(int(df.size * (1-train_size)))\n",
    "\n",
    "# Some global variables we'll need\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 512\n",
    "dim = df.shape[1]\n",
    "num_examples_to_generate = 256\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(train.values, tf.float32)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(dim, input_shape=(dim,)))\n",
    "    model.add(layers.BatchNormalization(momentum=.8))\n",
    "    model.add(layers.LeakyReLU(alpha=.2))\n",
    "\n",
    "    model.add(layers.Dense(2 * dim))\n",
    "    model.add(layers.BatchNormalization(momentum=.8))\n",
    "    model.add(layers.LeakyReLU(alpha=.2))\n",
    "\n",
    "    model.add(layers.Dense(4 * dim))\n",
    "    model.add(layers.BatchNormalization(momentum=.8))\n",
    "    model.add(layers.LeakyReLU(alpha=.2))\n",
    "    \n",
    "    model.add(layers.Dense(dim))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(dim, input_shape=(dim,)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(2 * dim))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(dim, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "generator = build_generator_model()\n",
    "discriminator = build_discriminator_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Losses, Optimizers, and Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(.00001)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_dir = './training_checkpoints'\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "# checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "#                                  discriminator_optimizer=discriminator_optimizer,\n",
    "#                                  generator=generator,\n",
    "#                                  discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(samples):\n",
    "    noise = tf.random.normal([BATCH_SIZE, dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_records = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(samples, training=True)\n",
    "      fake_output = discriminator(generated_records, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    for batch in dataset:\n",
    "      gen_loss, disc_loss = train_step(batch)\n",
    "\n",
    "    print(\"Generator Loss: \", gen_loss)\n",
    "    print(\"Discriminator Loss: \", disc_loss)\n",
    "    \n",
    "#     # Save the model every 15 epochs\n",
    "#     if (epoch + 1) % 15 == 0:\n",
    "#       checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss:  tf.Tensor(0.46864343, shape=(), dtype=float32)\n",
      "Discriminator Loss:  tf.Tensor(1.5161161, shape=(), dtype=float32)\n",
      "Time for epoch 1 is 364.94622468948364 sec\n",
      "Generator Loss:  tf.Tensor(0.4735946, shape=(), dtype=float32)\n",
      "Discriminator Loss:  tf.Tensor(1.5115825, shape=(), dtype=float32)\n",
      "Time for epoch 2 is 365.159321308136 sec\n",
      "Generator Loss:  tf.Tensor(0.4733104, shape=(), dtype=float32)\n",
      "Discriminator Loss:  tf.Tensor(1.5203242, shape=(), dtype=float32)\n",
      "Time for epoch 3 is 367.79504776000977 sec\n",
      "Generator Loss:  tf.Tensor(0.4773119, shape=(), dtype=float32)\n",
      "Discriminator Loss:  tf.Tensor(1.5176995, shape=(), dtype=float32)\n",
      "Time for epoch 4 is 365.05380964279175 sec\n",
      "Generator Loss:  tf.Tensor(0.48033404, shape=(), dtype=float32)\n",
      "Discriminator Loss:  tf.Tensor(1.5131345, shape=(), dtype=float32)\n",
      "Time for epoch 5 is 365.0112781524658 sec\n",
      "CPU times: user 30min 27s, sys: 47.4 ms, total: 30min 27s\n",
      "Wall time: 30min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.9252443  -4.2817936  -4.008192   ...  0.6407013   0.62792885\n",
      "  -3.4423625 ]\n",
      " [ 2.0405703  -3.3222964  -2.3516433  ... -0.2812785   0.15884714\n",
      "  -4.08027   ]\n",
      " [ 2.0428686  -5.302197   -2.4947195  ...  0.51397693  0.4424576\n",
      "  -3.7906888 ]\n",
      " [ 2.8683991  -4.1444473  -3.6069608  ...  0.34208918  0.2937318\n",
      "  -4.345035  ]\n",
      " [ 2.615412   -3.639512   -4.5312057  ...  0.11355054 -0.57811034\n",
      "  -4.23321   ]], shape=(5, 1044), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "res = generator(tf.random.normal([5, dim]))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
