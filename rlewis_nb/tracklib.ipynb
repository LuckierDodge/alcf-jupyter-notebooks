{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luckierdodge/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import datetime\n",
    "import heapq as heap\n",
    "from os.path import basename\n",
    "# Imports to use TensorFlow\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# Imports to graph t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "# Imports for xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Imports for Elapsed Time Prediction\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Pandas options\n",
    "pd.options.display.max_columns = 1200\n",
    "pd.options.display.max_rows = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Combine Tracklib and Cobalt DJC logs\n",
    "\n",
    "If you have already run Section 1, or have an existing copy of the combined `.csv`, skip to **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in TrackLib dataset\n",
    "libdf = pd.read_csv('../data/tracklib/tracklib_mira_20200227.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Cobalt logs dataset\n",
    "djc_df = pd.read_csv('../data/tracklib/mira_djc_complete/dim_job_composite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COBALT_JOBID</th>\n",
       "      <th>QUEUED_DATE_ID</th>\n",
       "      <th>START_DATE_ID</th>\n",
       "      <th>END_DATE_ID</th>\n",
       "      <th>WALLTIME_SECONDS</th>\n",
       "      <th>RUNTIME_SECONDS</th>\n",
       "      <th>NODES_USED</th>\n",
       "      <th>NODES_REQUESTED</th>\n",
       "      <th>CORES_USED</th>\n",
       "      <th>CORES_REQUESTED</th>\n",
       "      <th>...</th>\n",
       "      <th>gsl</th>\n",
       "      <th>netcdf</th>\n",
       "      <th>valgrind</th>\n",
       "      <th>essl_fftw</th>\n",
       "      <th>silo</th>\n",
       "      <th>bgclang</th>\n",
       "      <th>essl</th>\n",
       "      <th>darshan</th>\n",
       "      <th>scorep</th>\n",
       "      <th>metis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.763140e+05</td>\n",
       "      <td>2.763140e+05</td>\n",
       "      <td>2.763140e+05</td>\n",
       "      <td>2.763140e+05</td>\n",
       "      <td>276314.000000</td>\n",
       "      <td>276314.000000</td>\n",
       "      <td>276314.000000</td>\n",
       "      <td>276314.000000</td>\n",
       "      <td>276314.000000</td>\n",
       "      <td>276314.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "      <td>276314.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.046564e+06</td>\n",
       "      <td>2.016660e+07</td>\n",
       "      <td>2.016663e+07</td>\n",
       "      <td>2.016663e+07</td>\n",
       "      <td>11508.103838</td>\n",
       "      <td>7349.374773</td>\n",
       "      <td>2053.188300</td>\n",
       "      <td>2013.286880</td>\n",
       "      <td>32851.012804</td>\n",
       "      <td>32212.590082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.344312e+05</td>\n",
       "      <td>1.365261e+04</td>\n",
       "      <td>1.364564e+04</td>\n",
       "      <td>1.364466e+04</td>\n",
       "      <td>14334.547058</td>\n",
       "      <td>11750.518818</td>\n",
       "      <td>4310.332276</td>\n",
       "      <td>4307.036097</td>\n",
       "      <td>68965.316411</td>\n",
       "      <td>68912.577552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.522200e+05</td>\n",
       "      <td>2.014103e+07</td>\n",
       "      <td>2.014123e+07</td>\n",
       "      <td>2.015010e+07</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.183280e+05</td>\n",
       "      <td>2.015093e+07</td>\n",
       "      <td>2.015093e+07</td>\n",
       "      <td>2.015093e+07</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>425.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>8192.000000</td>\n",
       "      <td>8192.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.727955e+05</td>\n",
       "      <td>2.016120e+07</td>\n",
       "      <td>2.016121e+07</td>\n",
       "      <td>2.016121e+07</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>2666.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>8192.000000</td>\n",
       "      <td>8192.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.538465e+06</td>\n",
       "      <td>2.018051e+07</td>\n",
       "      <td>2.018051e+07</td>\n",
       "      <td>2.018051e+07</td>\n",
       "      <td>18000.000000</td>\n",
       "      <td>9563.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>32768.000000</td>\n",
       "      <td>32768.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.999000e+07</td>\n",
       "      <td>2.019073e+07</td>\n",
       "      <td>2.019073e+07</td>\n",
       "      <td>2.019073e+07</td>\n",
       "      <td>217200.000000</td>\n",
       "      <td>217243.000000</td>\n",
       "      <td>49152.000000</td>\n",
       "      <td>49152.000000</td>\n",
       "      <td>786432.000000</td>\n",
       "      <td>786432.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       COBALT_JOBID  QUEUED_DATE_ID  START_DATE_ID   END_DATE_ID  \\\n",
       "count  2.763140e+05    2.763140e+05   2.763140e+05  2.763140e+05   \n",
       "mean   1.046564e+06    2.016660e+07   2.016663e+07  2.016663e+07   \n",
       "std    5.344312e+05    1.365261e+04   1.364564e+04  1.364466e+04   \n",
       "min    3.522200e+05    2.014103e+07   2.014123e+07  2.015010e+07   \n",
       "25%    6.183280e+05    2.015093e+07   2.015093e+07  2.015093e+07   \n",
       "50%    9.727955e+05    2.016120e+07   2.016121e+07  2.016121e+07   \n",
       "75%    1.538465e+06    2.018051e+07   2.018051e+07  2.018051e+07   \n",
       "max    9.999000e+07    2.019073e+07   2.019073e+07  2.019073e+07   \n",
       "\n",
       "       WALLTIME_SECONDS  RUNTIME_SECONDS     NODES_USED  NODES_REQUESTED  \\\n",
       "count     276314.000000    276314.000000  276314.000000    276314.000000   \n",
       "mean       11508.103838      7349.374773    2053.188300      2013.286880   \n",
       "std        14334.547058     11750.518818    4310.332276      4307.036097   \n",
       "min          180.000000        15.000000       0.000000         1.000000   \n",
       "25%         2400.000000       425.000000     512.000000       512.000000   \n",
       "50%         6000.000000      2666.000000     512.000000       512.000000   \n",
       "75%        18000.000000      9563.000000    2048.000000      2048.000000   \n",
       "max       217200.000000    217243.000000   49152.000000     49152.000000   \n",
       "\n",
       "          CORES_USED  CORES_REQUESTED  ...       gsl    netcdf  valgrind  \\\n",
       "count  276314.000000    276314.000000  ...  276314.0  276314.0  276314.0   \n",
       "mean    32851.012804     32212.590082  ...       0.0       0.0       0.0   \n",
       "std     68965.316411     68912.577552  ...       0.0       0.0       0.0   \n",
       "min         0.000000        16.000000  ...       0.0       0.0       0.0   \n",
       "25%      8192.000000      8192.000000  ...       0.0       0.0       0.0   \n",
       "50%      8192.000000      8192.000000  ...       0.0       0.0       0.0   \n",
       "75%     32768.000000     32768.000000  ...       0.0       0.0       0.0   \n",
       "max    786432.000000    786432.000000  ...       0.0       0.0       0.0   \n",
       "\n",
       "       essl_fftw      silo   bgclang      essl   darshan    scorep     metis  \n",
       "count   276314.0  276314.0  276314.0  276314.0  276314.0  276314.0  276314.0  \n",
       "mean         0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "std          0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "min          0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "25%          0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "50%          0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "75%          0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "max          0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[8 rows x 80 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take cobalt logs and add a column for each short library name\n",
    "df = djc_df.assign(**dict.fromkeys(libdf.LIB_SHORT_NAME.unique(), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode library names into djc_df\n",
    "# THIS IS VERY INEFFICIENT, DON'T RUN UNLESS YOU NEED TO.\n",
    "# TAKES ~1 hour to run on flick ðŸ™ƒ\n",
    "for lib in libdf.LIB_SHORT_NAME.unique():\n",
    "    temp_df = libdf[libdf['LIB_SHORT_NAME'] == lib]\n",
    "    temp_df.set_index('COBALT_JOBID', inplace=True)\n",
    "    def helper(row):\n",
    "        if row['COBALT_JOBID'] in temp_df.index:\n",
    "            row[lib] = 1\n",
    "            return row\n",
    "        else:\n",
    "            return row\n",
    "    df = df.apply(helper, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write it out to .csv so we don't have to run this terribly inefficient\n",
    "# code every time\n",
    "df.to_csv('../data/tracklib/tracklib_djc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Clean and Filter Combined Dataset\n",
    "\n",
    "If you already have a filtered `.csv` you want to use, skip to **Section 3**.\n",
    "\n",
    "* Parameters:\n",
    "    * n\n",
    "        * Number of top corehours consumers to include\n",
    "        * Possible values: >=0\n",
    "        * Set to 0 to include all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the various dataframes if you need to\n",
    "df = pd.read_csv('../data/tracklib/tracklib_djc.csv', header=0, low_memory=False)\n",
    "libdf = pd.read_csv('../data/tracklib/tracklib_mira_20200227.csv')\n",
    "djc_df = pd.read_csv('../data/mira_djc_complete/dim_job_composite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Nans and weird column \n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter combined dataset based on date overlap\n",
    "df = df[df.START_TIMESTAMP > libdf.START_TIMESTAMP.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for an executable name/project name feature\n",
    "def exec_maker(row):\n",
    "    return str('%s.%s' % (row['PROJECT_NAME'], basename(row['COMMAND']))) \n",
    "\n",
    "df['EXEC'] = df.apply(exec_maker, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by top n core hour consumers\n",
    "# df_filtered = pd.DataFrame()\n",
    "# if n > 0:\n",
    "#     names = df.EXEC.unique()\n",
    "#     core_hours_dict = {}\n",
    "#     for name in names:\n",
    "#         core_hours_dict.update({name : df.USED_CORE_HOURS.sum()})\n",
    "#     top_names = heap.nlargest(n, core_hours_dict, key=core_hours_dict.get)\n",
    "#     for name in top_names:\n",
    "#         df_filtered = df_filtered.append(df[df['EXEC'] == name])\n",
    "# else:\n",
    "#     df_filtered = df.copy()\n",
    "\n",
    "n_top_execs = df.EXEC.value_counts().head(n - 1)\n",
    "df_filtered = df[df.EXEC.isin(n_top_execs.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a '.csv'\n",
    "if n > 0:\n",
    "    df_filtered.to_csv('../data/tracklib/tracklib_djc_filtered_' + str(n) + '.csv')\n",
    "else:\n",
    "    df_filtered.to_csv('../data/tracklib/tracklib_djc_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: TensorFlow Model\n",
    "\n",
    "* Parameters:\n",
    "    * m\n",
    "        * number of top corehours consumers to include.\n",
    "        * Possible values: >=0\n",
    "        * Set to 0 to include all of them. \n",
    "        * Set to n to use the value from **Section 2**.\n",
    "    * train_size\n",
    "        * how much of the full dataset to use for training\n",
    "        * Possible values: 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "m = 25\n",
    "train_size = .80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the various dataframes if you need to\n",
    "# df = pd.read_csv('../data/tracklib/tracklib_djc.csv')\n",
    "# libdf = pd.read_csv('../data/tracklib/tracklib_mira_20200227.csv')\n",
    "# djc_df = pd.read_csv('../data/mira_djc_complete/dim_job_composite.csv')\n",
    "if m > 0:\n",
    "    df_filtered = pd.read_csv('../data/tracklib/tracklib_djc_filtered_' + str(m) + '.csv')\n",
    "else:\n",
    "    df_filtered = pd.read_csv('../data/tracklib/tracklib_djc_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and targets to use for model\n",
    "features = df_filtered.select_dtypes(include=[np.number]).copy()\n",
    "features = features.drop([column for column in features.columns if features[column].max() == features[column].min()], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'ID' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'unnamed' in column.lower()], axis=1)\n",
    "\n",
    "targets = pd.DataFrame()\n",
    "targets['number'] = pd.factorize(df_filtered['EXEC'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXPERIMENT: Drop everthing except libraries\n",
    "# features = features.drop([column for column in features.columns if 'NUM' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'SECONDS' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'USED' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'CORE' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'NODE' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'IS' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'EXIT' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'FACTOR' in column], axis=1)\n",
    "\n",
    "# for columns in features.columns:\n",
    "#     print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXPERIMENT: Test without libraries\n",
    "# features = features.drop([column for column in features.columns if column.islower()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WALLTIME_SECONDS', 'NODES_REQUESTED', 'CORES_REQUESTED',\n",
       "       'REQUESTED_CORE_HOURS', 'OVERBURN_CORE_HOURS', 'bgsys-gnu-linux',\n",
       "       'bgsys-gnu-linux-4.7.2', 'autoperf', 'bgsys-comm', 'bgsys-hwi',\n",
       "       'ibmcmp', 'bgsys-bgpm', 'bgsys-spi', 'qmcpack', 'bgsys-cnk', 'fftw',\n",
       "       'zlib', 'hdf5', 'lapack', 'bgclang', 'essl', 'darshan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXPERIMENT: Drop features we don't have access to before runtime\n",
    "features = features.drop([column for column in features.columns if 'RUNTIME_SECONDS' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'NODES_USED' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'CORES_USED' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'EXIT_STATUS' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'USED_CORE_HOURS' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'COBALT_NUM_TASKS' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'ELIGIBLE' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'QUEUED' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'USAGE' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'EXIT_CODE' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'NUM_TASK' in column], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'IS_' in column], axis=1)\n",
    "\n",
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features between 0 and 1 - plays nice with our neural network\n",
    "# features = ((features - features.mean()) / (features.max() - features.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate our data into training and test sets\n",
    "train_examples = features.head(int(len(df_filtered.index) * train_size))\n",
    "train_targets = targets.head(int(len(df_filtered.index) * train_size))\n",
    "test_examples = features.tail(int(len(df_filtered.index) * (1-train_size)))\n",
    "test_targets = targets.tail(int(len(df_filtered.index) * (1-train_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luckierdodge/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Define our TensorFlow model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(256, activation='relu', input_shape=(len(features.columns.values),)),\n",
    "  tf.keras.layers.Dense(128, activation='relu', input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(128, activation='relu', input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(128, activation='relu', input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(targets.number.unique().size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile our model\n",
    "model.compile(optimizer=tf.train.AdagradOptimizer(0.05),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luckierdodge/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 47044 samples\n",
      "Epoch 1/10\n",
      "47044/47044 [==============================] - 3s 56us/sample - loss: 239.5653 - sparse_categorical_accuracy: 0.1297\n",
      "Epoch 2/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.9880 - sparse_categorical_accuracy: 0.1455\n",
      "Epoch 3/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.7578 - sparse_categorical_accuracy: 0.2025\n",
      "Epoch 4/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.6550 - sparse_categorical_accuracy: 0.2187\n",
      "Epoch 5/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.6119 - sparse_categorical_accuracy: 0.2265\n",
      "Epoch 6/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.6083 - sparse_categorical_accuracy: 0.2309\n",
      "Epoch 7/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.5885 - sparse_categorical_accuracy: 0.2367\n",
      "Epoch 8/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.5647 - sparse_categorical_accuracy: 0.2412\n",
      "Epoch 9/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.5454 - sparse_categorical_accuracy: 0.2433\n",
      "Epoch 10/10\n",
      "47044/47044 [==============================] - 2s 40us/sample - loss: 2.5231 - sparse_categorical_accuracy: 0.2452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f21906fbf50>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(train_examples,\n",
    "          train_targets,\n",
    "          epochs=10,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11760/11760 [==============================] - 0s 21us/sample - loss: 2.5017 - sparse_categorical_accuracy: 0.2456\n",
      "Test accuracy: [2.5017234156731845, 0.24557823]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Test Accuracy to make sure we haven't overfit\n",
    "test_acc = model.evaluate(test_examples, test_targets)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: T-SNE Model\n",
    "\n",
    "Use Zhengchung's t-sne plotting code to make a half-decent plot of a T-SNE representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the various dataframes if you need to\n",
    "# df = pd.read_csv('../data/tracklib/tracklib_djc.csv')\n",
    "# libdf = pd.read_csv('../data/tracklib/tracklib_mira_20200227.csv')\n",
    "# djc_df = pd.read_csv('../data/mira_djc_complete/dim_job_composite.csv')\n",
    "if m > 0:\n",
    "    df_filtered = pd.read_csv('../data/tracklib/tracklib_djc_filtered_' + str(m) + '.csv')\n",
    "else:\n",
    "    df_filtered = pd.read_csv('../data/tracklib/tracklib_djc_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_filtered.select_dtypes(include=[np.number]).copy()\n",
    "features = features.drop([column for column in features.columns if features[column].max() == features[column].min()], axis=1)\n",
    "features = features.drop([column for column in features.columns if 'ID' in column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-e3c8c978022f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train T-SNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_embedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \"\"\"\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    796\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     def _tsne(self, P, degrees_of_freedom, n_samples, X_embedded,\n",
      "\u001b[0;32m~/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_iter_without_progress'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_without_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m             params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0;32m--> 852\u001b[0;31m                                                           **opt_args)\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;31m# Save the final number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compute_error'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_convergence\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyterlab/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[1;32m    260\u001b[0m                                       \u001b[0mdof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                                       \u001b[0mcompute_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                                       num_threads=num_threads)\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train T-SNE\n",
    "X_embedded = TSNE().fit_transform(features)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_sne_vis_by_group(x_emb, exe_idn, topn=10):\n",
    "    idn_unique, idn_count = np.unique(exe_idn, return_counts=True)\n",
    "    top_idn = idn_unique[np.argsort(idn_count)[-topn:]]\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    colors = ('g', 'b', 'gold', 'yellow', 'tan', 'cyan', 'magenta', 'black', 'orange', 'darkgreen')\n",
    "    markers= ('x', 'o', '>', '<', 's', 'v', 'H', 'D', '3', '1', '2')\n",
    "    _other_grp = np.zeros(exe_idn.shape[0], dtype=np.bool)\n",
    "    for _idx, _idn in enumerate(top_idn):\n",
    "        _emb_grp = x_emb[exe_idn == _idn]\n",
    "        plt.plot(_emb_grp[:, 0], _emb_grp[:, 1], markers[_idx], alpha=.8, color=colors[_idx], markersize=6, label = _idn)\n",
    "        _other_grp |= (exe_idn == _idn)\n",
    "#         print(\"%s is marked by %s and %s\" % (_idn, colors[_idx], markers[_idx]))\n",
    "    _uncat = x_emb[~_other_grp]\n",
    "    plt.plot(_uncat[:, 0], _uncat[:, 1], markers[-1], alpha=.8, color='r', markersize=6, label = 'Others')\n",
    "    \n",
    "    plt.xlim(left=X_embedded[:, 0].min()*1.05, right=X_embedded[:, 0].max()*1.05)\n",
    "    plt.ylim(bottom=X_embedded[:, 1].min()*1.05, top=X_embedded[:, 1].max()*1.05)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.legend(bbox_to_anchor=(0., 1.0, 1., .102), ncol=4, loc=3, fancybox=True, framealpha=0.5, fontsize=14)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "t_sne_vis_by_group(X_embedded, df_filtered['EXEC'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "m = 25\n",
    "train_size = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the various dataframes if you need to\n",
    "# df = pd.read_csv('../data/tracklib/tracklib_djc.csv')\n",
    "# libdf = pd.read_csv('../data/tracklib/tracklib_mira_20200227.csv')\n",
    "# djc_df = pd.read_csv('../data/mira_djc_complete/dim_job_composite.csv')\n",
    "if m > 0:\n",
    "    df_filtered = pd.read_csv('../data/tracklib/tracklib_djc_filtered_' + str(m) + '.csv')\n",
    "else:\n",
    "    df_filtered = pd.read_csv('../data/tracklib/tracklib_djc_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[df_filtered.WALLTIME_SECONDS >= df_filtered.RUNTIME_SECONDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[df_filtered['EXIT_STATUS'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42678"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_filtered.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "features['WALLTIME_SECONDS'] = df_filtered['WALLTIME_SECONDS']\n",
    "for column in df_filtered.columns:\n",
    "    if column.islower():\n",
    "        features[column] = df_filtered[column]\n",
    "\n",
    "cols = ['USERNAME', 'PROJECT_NAME', 'QUEUE_NAME', 'EXEC', 'NODES_REQUESTED', 'REQUESTED_CORE_HOURS']\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False, categories='auto')\n",
    "enc.fit(df_filtered[cols])\n",
    "features = pd.concat([features.reset_index(drop=True), pd.DataFrame(enc.transform(df_filtered[cols]))], axis=1)\n",
    "# features = pd.concat([features, pd.DataFrame(enc.transform(df_filtered[cols])).reindex(features.index)], axis=1)\n",
    "\n",
    "targets = df_filtered.RUNTIME_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WALLTIME_SECONDS</th>\n",
       "      <th>boost</th>\n",
       "      <th>bgsys-gnu-linux</th>\n",
       "      <th>bgsys-gnu-linux-4.7.2</th>\n",
       "      <th>autoperf</th>\n",
       "      <th>paraview</th>\n",
       "      <th>parmetis</th>\n",
       "      <th>blas</th>\n",
       "      <th>bgsys-comm</th>\n",
       "      <th>bgsys-hwi</th>\n",
       "      <th>ibmcmp</th>\n",
       "      <th>bgsys-bgpm</th>\n",
       "      <th>bgsys-spi</th>\n",
       "      <th>python</th>\n",
       "      <th>qmcpack</th>\n",
       "      <th>pnetcdf</th>\n",
       "      <th>papi</th>\n",
       "      <th>petsc</th>\n",
       "      <th>bgsys-cnk</th>\n",
       "      <th>fftw</th>\n",
       "      <th>zlib</th>\n",
       "      <th>scalapack</th>\n",
       "      <th>hdf5</th>\n",
       "      <th>hpctw</th>\n",
       "      <th>szip</th>\n",
       "      <th>mxml</th>\n",
       "      <th>gcc</th>\n",
       "      <th>lapack</th>\n",
       "      <th>gsl</th>\n",
       "      <th>netcdf</th>\n",
       "      <th>valgrind</th>\n",
       "      <th>essl_fftw</th>\n",
       "      <th>silo</th>\n",
       "      <th>bgclang</th>\n",
       "      <th>essl</th>\n",
       "      <th>darshan</th>\n",
       "      <th>scorep</th>\n",
       "      <th>metis</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>415</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>459</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "      <th>514</th>\n",
       "      <th>515</th>\n",
       "      <th>516</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "      <th>527</th>\n",
       "      <th>528</th>\n",
       "      <th>529</th>\n",
       "      <th>530</th>\n",
       "      <th>531</th>\n",
       "      <th>532</th>\n",
       "      <th>533</th>\n",
       "      <th>534</th>\n",
       "      <th>535</th>\n",
       "      <th>536</th>\n",
       "      <th>537</th>\n",
       "      <th>538</th>\n",
       "      <th>539</th>\n",
       "      <th>540</th>\n",
       "      <th>541</th>\n",
       "      <th>542</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "      <th>545</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>...</th>\n",
       "      <th>10136</th>\n",
       "      <th>10137</th>\n",
       "      <th>10138</th>\n",
       "      <th>10139</th>\n",
       "      <th>10140</th>\n",
       "      <th>10141</th>\n",
       "      <th>10142</th>\n",
       "      <th>10143</th>\n",
       "      <th>10144</th>\n",
       "      <th>10145</th>\n",
       "      <th>10146</th>\n",
       "      <th>10147</th>\n",
       "      <th>10148</th>\n",
       "      <th>10149</th>\n",
       "      <th>10150</th>\n",
       "      <th>10151</th>\n",
       "      <th>10152</th>\n",
       "      <th>10153</th>\n",
       "      <th>10154</th>\n",
       "      <th>10155</th>\n",
       "      <th>10156</th>\n",
       "      <th>10157</th>\n",
       "      <th>10158</th>\n",
       "      <th>10159</th>\n",
       "      <th>10160</th>\n",
       "      <th>10161</th>\n",
       "      <th>10162</th>\n",
       "      <th>10163</th>\n",
       "      <th>10164</th>\n",
       "      <th>10165</th>\n",
       "      <th>10166</th>\n",
       "      <th>10167</th>\n",
       "      <th>10168</th>\n",
       "      <th>10169</th>\n",
       "      <th>10170</th>\n",
       "      <th>10171</th>\n",
       "      <th>10172</th>\n",
       "      <th>10173</th>\n",
       "      <th>10174</th>\n",
       "      <th>10175</th>\n",
       "      <th>10176</th>\n",
       "      <th>10177</th>\n",
       "      <th>10178</th>\n",
       "      <th>10179</th>\n",
       "      <th>10180</th>\n",
       "      <th>10181</th>\n",
       "      <th>10182</th>\n",
       "      <th>10183</th>\n",
       "      <th>10184</th>\n",
       "      <th>10185</th>\n",
       "      <th>10186</th>\n",
       "      <th>10187</th>\n",
       "      <th>10188</th>\n",
       "      <th>10189</th>\n",
       "      <th>10190</th>\n",
       "      <th>10191</th>\n",
       "      <th>10192</th>\n",
       "      <th>10193</th>\n",
       "      <th>10194</th>\n",
       "      <th>10195</th>\n",
       "      <th>10196</th>\n",
       "      <th>10197</th>\n",
       "      <th>10198</th>\n",
       "      <th>10199</th>\n",
       "      <th>10200</th>\n",
       "      <th>10201</th>\n",
       "      <th>10202</th>\n",
       "      <th>10203</th>\n",
       "      <th>10204</th>\n",
       "      <th>10205</th>\n",
       "      <th>10206</th>\n",
       "      <th>10207</th>\n",
       "      <th>10208</th>\n",
       "      <th>10209</th>\n",
       "      <th>10210</th>\n",
       "      <th>10211</th>\n",
       "      <th>10212</th>\n",
       "      <th>10213</th>\n",
       "      <th>10214</th>\n",
       "      <th>10215</th>\n",
       "      <th>10216</th>\n",
       "      <th>10217</th>\n",
       "      <th>10218</th>\n",
       "      <th>10219</th>\n",
       "      <th>10220</th>\n",
       "      <th>10221</th>\n",
       "      <th>10222</th>\n",
       "      <th>10223</th>\n",
       "      <th>10224</th>\n",
       "      <th>10225</th>\n",
       "      <th>10226</th>\n",
       "      <th>10227</th>\n",
       "      <th>10228</th>\n",
       "      <th>10229</th>\n",
       "      <th>10230</th>\n",
       "      <th>10231</th>\n",
       "      <th>10232</th>\n",
       "      <th>10233</th>\n",
       "      <th>10234</th>\n",
       "      <th>10235</th>\n",
       "      <th>10236</th>\n",
       "      <th>10237</th>\n",
       "      <th>10238</th>\n",
       "      <th>10239</th>\n",
       "      <th>10240</th>\n",
       "      <th>10241</th>\n",
       "      <th>10242</th>\n",
       "      <th>10243</th>\n",
       "      <th>10244</th>\n",
       "      <th>10245</th>\n",
       "      <th>10246</th>\n",
       "      <th>10247</th>\n",
       "      <th>10248</th>\n",
       "      <th>10249</th>\n",
       "      <th>10250</th>\n",
       "      <th>10251</th>\n",
       "      <th>10252</th>\n",
       "      <th>10253</th>\n",
       "      <th>10254</th>\n",
       "      <th>10255</th>\n",
       "      <th>10256</th>\n",
       "      <th>10257</th>\n",
       "      <th>10258</th>\n",
       "      <th>10259</th>\n",
       "      <th>10260</th>\n",
       "      <th>10261</th>\n",
       "      <th>10262</th>\n",
       "      <th>10263</th>\n",
       "      <th>10264</th>\n",
       "      <th>10265</th>\n",
       "      <th>10266</th>\n",
       "      <th>10267</th>\n",
       "      <th>10268</th>\n",
       "      <th>10269</th>\n",
       "      <th>10270</th>\n",
       "      <th>10271</th>\n",
       "      <th>10272</th>\n",
       "      <th>10273</th>\n",
       "      <th>10274</th>\n",
       "      <th>10275</th>\n",
       "      <th>10276</th>\n",
       "      <th>10277</th>\n",
       "      <th>10278</th>\n",
       "      <th>10279</th>\n",
       "      <th>10280</th>\n",
       "      <th>10281</th>\n",
       "      <th>10282</th>\n",
       "      <th>10283</th>\n",
       "      <th>10284</th>\n",
       "      <th>10285</th>\n",
       "      <th>10286</th>\n",
       "      <th>10287</th>\n",
       "      <th>10288</th>\n",
       "      <th>10289</th>\n",
       "      <th>10290</th>\n",
       "      <th>10291</th>\n",
       "      <th>10292</th>\n",
       "      <th>10293</th>\n",
       "      <th>10294</th>\n",
       "      <th>10295</th>\n",
       "      <th>10296</th>\n",
       "      <th>10297</th>\n",
       "      <th>10298</th>\n",
       "      <th>10299</th>\n",
       "      <th>10300</th>\n",
       "      <th>10301</th>\n",
       "      <th>10302</th>\n",
       "      <th>10303</th>\n",
       "      <th>10304</th>\n",
       "      <th>10305</th>\n",
       "      <th>10306</th>\n",
       "      <th>10307</th>\n",
       "      <th>10308</th>\n",
       "      <th>10309</th>\n",
       "      <th>10310</th>\n",
       "      <th>10311</th>\n",
       "      <th>10312</th>\n",
       "      <th>10313</th>\n",
       "      <th>10314</th>\n",
       "      <th>10315</th>\n",
       "      <th>10316</th>\n",
       "      <th>10317</th>\n",
       "      <th>10318</th>\n",
       "      <th>10319</th>\n",
       "      <th>10320</th>\n",
       "      <th>10321</th>\n",
       "      <th>10322</th>\n",
       "      <th>10323</th>\n",
       "      <th>10324</th>\n",
       "      <th>10325</th>\n",
       "      <th>10326</th>\n",
       "      <th>10327</th>\n",
       "      <th>10328</th>\n",
       "      <th>10329</th>\n",
       "      <th>10330</th>\n",
       "      <th>10331</th>\n",
       "      <th>10332</th>\n",
       "      <th>10333</th>\n",
       "      <th>10334</th>\n",
       "      <th>10335</th>\n",
       "      <th>10336</th>\n",
       "      <th>10337</th>\n",
       "      <th>10338</th>\n",
       "      <th>10339</th>\n",
       "      <th>10340</th>\n",
       "      <th>10341</th>\n",
       "      <th>10342</th>\n",
       "      <th>10343</th>\n",
       "      <th>10344</th>\n",
       "      <th>10345</th>\n",
       "      <th>10346</th>\n",
       "      <th>10347</th>\n",
       "      <th>10348</th>\n",
       "      <th>10349</th>\n",
       "      <th>10350</th>\n",
       "      <th>10351</th>\n",
       "      <th>10352</th>\n",
       "      <th>10353</th>\n",
       "      <th>10354</th>\n",
       "      <th>10355</th>\n",
       "      <th>10356</th>\n",
       "      <th>10357</th>\n",
       "      <th>10358</th>\n",
       "      <th>10359</th>\n",
       "      <th>10360</th>\n",
       "      <th>10361</th>\n",
       "      <th>10362</th>\n",
       "      <th>10363</th>\n",
       "      <th>10364</th>\n",
       "      <th>10365</th>\n",
       "      <th>10366</th>\n",
       "      <th>10367</th>\n",
       "      <th>10368</th>\n",
       "      <th>10369</th>\n",
       "      <th>10370</th>\n",
       "      <th>10371</th>\n",
       "      <th>10372</th>\n",
       "      <th>10373</th>\n",
       "      <th>10374</th>\n",
       "      <th>10375</th>\n",
       "      <th>10376</th>\n",
       "      <th>10377</th>\n",
       "      <th>10378</th>\n",
       "      <th>10379</th>\n",
       "      <th>10380</th>\n",
       "      <th>10381</th>\n",
       "      <th>10382</th>\n",
       "      <th>10383</th>\n",
       "      <th>10384</th>\n",
       "      <th>10385</th>\n",
       "      <th>10386</th>\n",
       "      <th>10387</th>\n",
       "      <th>10388</th>\n",
       "      <th>10389</th>\n",
       "      <th>10390</th>\n",
       "      <th>10391</th>\n",
       "      <th>10392</th>\n",
       "      <th>10393</th>\n",
       "      <th>10394</th>\n",
       "      <th>10395</th>\n",
       "      <th>10396</th>\n",
       "      <th>10397</th>\n",
       "      <th>10398</th>\n",
       "      <th>10399</th>\n",
       "      <th>10400</th>\n",
       "      <th>10401</th>\n",
       "      <th>10402</th>\n",
       "      <th>10403</th>\n",
       "      <th>10404</th>\n",
       "      <th>10405</th>\n",
       "      <th>10406</th>\n",
       "      <th>10407</th>\n",
       "      <th>10408</th>\n",
       "      <th>10409</th>\n",
       "      <th>10410</th>\n",
       "      <th>10411</th>\n",
       "      <th>10412</th>\n",
       "      <th>10413</th>\n",
       "      <th>10414</th>\n",
       "      <th>10415</th>\n",
       "      <th>10416</th>\n",
       "      <th>10417</th>\n",
       "      <th>10418</th>\n",
       "      <th>10419</th>\n",
       "      <th>10420</th>\n",
       "      <th>10421</th>\n",
       "      <th>10422</th>\n",
       "      <th>10423</th>\n",
       "      <th>10424</th>\n",
       "      <th>10425</th>\n",
       "      <th>10426</th>\n",
       "      <th>10427</th>\n",
       "      <th>10428</th>\n",
       "      <th>10429</th>\n",
       "      <th>10430</th>\n",
       "      <th>10431</th>\n",
       "      <th>10432</th>\n",
       "      <th>10433</th>\n",
       "      <th>10434</th>\n",
       "      <th>10435</th>\n",
       "      <th>10436</th>\n",
       "      <th>10437</th>\n",
       "      <th>10438</th>\n",
       "      <th>10439</th>\n",
       "      <th>10440</th>\n",
       "      <th>10441</th>\n",
       "      <th>10442</th>\n",
       "      <th>10443</th>\n",
       "      <th>10444</th>\n",
       "      <th>10445</th>\n",
       "      <th>10446</th>\n",
       "      <th>10447</th>\n",
       "      <th>10448</th>\n",
       "      <th>10449</th>\n",
       "      <th>10450</th>\n",
       "      <th>10451</th>\n",
       "      <th>10452</th>\n",
       "      <th>10453</th>\n",
       "      <th>10454</th>\n",
       "      <th>10455</th>\n",
       "      <th>10456</th>\n",
       "      <th>10457</th>\n",
       "      <th>10458</th>\n",
       "      <th>10459</th>\n",
       "      <th>10460</th>\n",
       "      <th>10461</th>\n",
       "      <th>10462</th>\n",
       "      <th>10463</th>\n",
       "      <th>10464</th>\n",
       "      <th>10465</th>\n",
       "      <th>10466</th>\n",
       "      <th>10467</th>\n",
       "      <th>10468</th>\n",
       "      <th>10469</th>\n",
       "      <th>10470</th>\n",
       "      <th>10471</th>\n",
       "      <th>10472</th>\n",
       "      <th>10473</th>\n",
       "      <th>10474</th>\n",
       "      <th>10475</th>\n",
       "      <th>10476</th>\n",
       "      <th>10477</th>\n",
       "      <th>10478</th>\n",
       "      <th>10479</th>\n",
       "      <th>10480</th>\n",
       "      <th>10481</th>\n",
       "      <th>10482</th>\n",
       "      <th>10483</th>\n",
       "      <th>10484</th>\n",
       "      <th>10485</th>\n",
       "      <th>10486</th>\n",
       "      <th>10487</th>\n",
       "      <th>10488</th>\n",
       "      <th>10489</th>\n",
       "      <th>10490</th>\n",
       "      <th>10491</th>\n",
       "      <th>10492</th>\n",
       "      <th>10493</th>\n",
       "      <th>10494</th>\n",
       "      <th>10495</th>\n",
       "      <th>10496</th>\n",
       "      <th>10497</th>\n",
       "      <th>10498</th>\n",
       "      <th>10499</th>\n",
       "      <th>10500</th>\n",
       "      <th>10501</th>\n",
       "      <th>10502</th>\n",
       "      <th>10503</th>\n",
       "      <th>10504</th>\n",
       "      <th>10505</th>\n",
       "      <th>10506</th>\n",
       "      <th>10507</th>\n",
       "      <th>10508</th>\n",
       "      <th>10509</th>\n",
       "      <th>10510</th>\n",
       "      <th>10511</th>\n",
       "      <th>10512</th>\n",
       "      <th>10513</th>\n",
       "      <th>10514</th>\n",
       "      <th>10515</th>\n",
       "      <th>10516</th>\n",
       "      <th>10517</th>\n",
       "      <th>10518</th>\n",
       "      <th>10519</th>\n",
       "      <th>10520</th>\n",
       "      <th>10521</th>\n",
       "      <th>10522</th>\n",
       "      <th>10523</th>\n",
       "      <th>10524</th>\n",
       "      <th>10525</th>\n",
       "      <th>10526</th>\n",
       "      <th>10527</th>\n",
       "      <th>10528</th>\n",
       "      <th>10529</th>\n",
       "      <th>10530</th>\n",
       "      <th>10531</th>\n",
       "      <th>10532</th>\n",
       "      <th>10533</th>\n",
       "      <th>10534</th>\n",
       "      <th>10535</th>\n",
       "      <th>10536</th>\n",
       "      <th>10537</th>\n",
       "      <th>10538</th>\n",
       "      <th>10539</th>\n",
       "      <th>10540</th>\n",
       "      <th>10541</th>\n",
       "      <th>10542</th>\n",
       "      <th>10543</th>\n",
       "      <th>10544</th>\n",
       "      <th>10545</th>\n",
       "      <th>10546</th>\n",
       "      <th>10547</th>\n",
       "      <th>10548</th>\n",
       "      <th>10549</th>\n",
       "      <th>10550</th>\n",
       "      <th>10551</th>\n",
       "      <th>10552</th>\n",
       "      <th>10553</th>\n",
       "      <th>10554</th>\n",
       "      <th>10555</th>\n",
       "      <th>10556</th>\n",
       "      <th>10557</th>\n",
       "      <th>10558</th>\n",
       "      <th>10559</th>\n",
       "      <th>10560</th>\n",
       "      <th>10561</th>\n",
       "      <th>10562</th>\n",
       "      <th>10563</th>\n",
       "      <th>10564</th>\n",
       "      <th>10565</th>\n",
       "      <th>10566</th>\n",
       "      <th>10567</th>\n",
       "      <th>10568</th>\n",
       "      <th>10569</th>\n",
       "      <th>10570</th>\n",
       "      <th>10571</th>\n",
       "      <th>10572</th>\n",
       "      <th>10573</th>\n",
       "      <th>10574</th>\n",
       "      <th>10575</th>\n",
       "      <th>10576</th>\n",
       "      <th>10577</th>\n",
       "      <th>10578</th>\n",
       "      <th>10579</th>\n",
       "      <th>10580</th>\n",
       "      <th>10581</th>\n",
       "      <th>10582</th>\n",
       "      <th>10583</th>\n",
       "      <th>10584</th>\n",
       "      <th>10585</th>\n",
       "      <th>10586</th>\n",
       "      <th>10587</th>\n",
       "      <th>10588</th>\n",
       "      <th>10589</th>\n",
       "      <th>10590</th>\n",
       "      <th>10591</th>\n",
       "      <th>10592</th>\n",
       "      <th>10593</th>\n",
       "      <th>10594</th>\n",
       "      <th>10595</th>\n",
       "      <th>10596</th>\n",
       "      <th>10597</th>\n",
       "      <th>10598</th>\n",
       "      <th>10599</th>\n",
       "      <th>10600</th>\n",
       "      <th>10601</th>\n",
       "      <th>10602</th>\n",
       "      <th>10603</th>\n",
       "      <th>10604</th>\n",
       "      <th>10605</th>\n",
       "      <th>10606</th>\n",
       "      <th>10607</th>\n",
       "      <th>10608</th>\n",
       "      <th>10609</th>\n",
       "      <th>10610</th>\n",
       "      <th>10611</th>\n",
       "      <th>10612</th>\n",
       "      <th>10613</th>\n",
       "      <th>10614</th>\n",
       "      <th>10615</th>\n",
       "      <th>10616</th>\n",
       "      <th>10617</th>\n",
       "      <th>10618</th>\n",
       "      <th>10619</th>\n",
       "      <th>10620</th>\n",
       "      <th>10621</th>\n",
       "      <th>10622</th>\n",
       "      <th>10623</th>\n",
       "      <th>10624</th>\n",
       "      <th>10625</th>\n",
       "      <th>10626</th>\n",
       "      <th>10627</th>\n",
       "      <th>10628</th>\n",
       "      <th>10629</th>\n",
       "      <th>10630</th>\n",
       "      <th>10631</th>\n",
       "      <th>10632</th>\n",
       "      <th>10633</th>\n",
       "      <th>10634</th>\n",
       "      <th>10635</th>\n",
       "      <th>10636</th>\n",
       "      <th>10637</th>\n",
       "      <th>10638</th>\n",
       "      <th>10639</th>\n",
       "      <th>10640</th>\n",
       "      <th>10641</th>\n",
       "      <th>10642</th>\n",
       "      <th>10643</th>\n",
       "      <th>10644</th>\n",
       "      <th>10645</th>\n",
       "      <th>10646</th>\n",
       "      <th>10647</th>\n",
       "      <th>10648</th>\n",
       "      <th>10649</th>\n",
       "      <th>10650</th>\n",
       "      <th>10651</th>\n",
       "      <th>10652</th>\n",
       "      <th>10653</th>\n",
       "      <th>10654</th>\n",
       "      <th>10655</th>\n",
       "      <th>10656</th>\n",
       "      <th>10657</th>\n",
       "      <th>10658</th>\n",
       "      <th>10659</th>\n",
       "      <th>10660</th>\n",
       "      <th>10661</th>\n",
       "      <th>10662</th>\n",
       "      <th>10663</th>\n",
       "      <th>10664</th>\n",
       "      <th>10665</th>\n",
       "      <th>10666</th>\n",
       "      <th>10667</th>\n",
       "      <th>10668</th>\n",
       "      <th>10669</th>\n",
       "      <th>10670</th>\n",
       "      <th>10671</th>\n",
       "      <th>10672</th>\n",
       "      <th>10673</th>\n",
       "      <th>10674</th>\n",
       "      <th>10675</th>\n",
       "      <th>10676</th>\n",
       "      <th>10677</th>\n",
       "      <th>10678</th>\n",
       "      <th>10679</th>\n",
       "      <th>10680</th>\n",
       "      <th>10681</th>\n",
       "      <th>10682</th>\n",
       "      <th>10683</th>\n",
       "      <th>10684</th>\n",
       "      <th>10685</th>\n",
       "      <th>10686</th>\n",
       "      <th>10687</th>\n",
       "      <th>10688</th>\n",
       "      <th>10689</th>\n",
       "      <th>10690</th>\n",
       "      <th>10691</th>\n",
       "      <th>10692</th>\n",
       "      <th>10693</th>\n",
       "      <th>10694</th>\n",
       "      <th>10695</th>\n",
       "      <th>10696</th>\n",
       "      <th>10697</th>\n",
       "      <th>10698</th>\n",
       "      <th>10699</th>\n",
       "      <th>10700</th>\n",
       "      <th>10701</th>\n",
       "      <th>10702</th>\n",
       "      <th>10703</th>\n",
       "      <th>10704</th>\n",
       "      <th>10705</th>\n",
       "      <th>10706</th>\n",
       "      <th>10707</th>\n",
       "      <th>10708</th>\n",
       "      <th>10709</th>\n",
       "      <th>10710</th>\n",
       "      <th>10711</th>\n",
       "      <th>10712</th>\n",
       "      <th>10713</th>\n",
       "      <th>10714</th>\n",
       "      <th>10715</th>\n",
       "      <th>10716</th>\n",
       "      <th>10717</th>\n",
       "      <th>10718</th>\n",
       "      <th>10719</th>\n",
       "      <th>10720</th>\n",
       "      <th>10721</th>\n",
       "      <th>10722</th>\n",
       "      <th>10723</th>\n",
       "      <th>10724</th>\n",
       "      <th>10725</th>\n",
       "      <th>10726</th>\n",
       "      <th>10727</th>\n",
       "      <th>10728</th>\n",
       "      <th>10729</th>\n",
       "      <th>10730</th>\n",
       "      <th>10731</th>\n",
       "      <th>10732</th>\n",
       "      <th>10733</th>\n",
       "      <th>10734</th>\n",
       "      <th>10735</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 10774 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [WALLTIME_SECONDS, boost, bgsys-gnu-linux, bgsys-gnu-linux-4.7.2, autoperf, paraview, parmetis, blas, bgsys-comm, bgsys-hwi, ibmcmp, bgsys-bgpm, bgsys-spi, python, qmcpack, pnetcdf, papi, petsc, bgsys-cnk, fftw, zlib, scalapack, hdf5, hpctw, szip, mxml, gcc, lapack, gsl, netcdf, valgrind, essl_fftw, silo, bgclang, essl, darshan, scorep, metis, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 10774 columns]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[features.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42678"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and targets to use for model\n",
    "# features = df_filtered.select_dtypes(include=[np.number]).copy()\n",
    "# features = features.drop([column for column in features.columns if features[column].max() == features[column].min()], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'ID' in column], axis=1)\n",
    "# features = features.drop([column for column in features.columns if 'unnamed' in column.lower()], axis=1)\n",
    "\n",
    "# targets = df_filtered.RUNTIME_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXPERIMENT: Test without libraries\n",
    "# features = features.drop([column for column in features.columns if str(column).islower()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = LabelBinarizer()\n",
    "# enc.fit(df_filtered.EXEC.values.reshape(-1, 1))\n",
    "# encoding = enc.transform(df_filtered.EXEC.values.reshape(-1,1))\n",
    "# features = pd.concat([features, pd.DataFrame(encoding)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = features.head(int(len(df_filtered.index) * train_size))\n",
    "train_targets = targets.head(int(len(df_filtered.index) * train_size))\n",
    "test_examples = features.tail(int(len(df_filtered.index) * (1-train_size)))\n",
    "test_targets = targets.tail(int(len(df_filtered.index) * (1-train_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", max_depth=10)\n",
    "# xgb_model.fit(train_examples, train_targets.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(test_targets, xgb_model.predict(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elapsed Time Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 18.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 18.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                    colsample_bylevel=None,\n",
       "                                    colsample_bynode=None,\n",
       "                                    colsample_bytree=None, gamma=None,\n",
       "                                    gpu_id=None, importance_type='gain',\n",
       "                                    interaction_constraints=None,\n",
       "                                    learning_rate=None, max_delta_step=None,\n",
       "                                    max_depth=None, min_child_weight=None,\n",
       "                                    missing=nan, monotone_constraints=None,\n",
       "                                    n_estima...\n",
       "                                    num_parallel_tree=None,\n",
       "                                    objective='reg:squarederror',\n",
       "                                    random_state=None, reg_alpha=None,\n",
       "                                    reg_lambda=None, scale_pos_weight=None,\n",
       "                                    subsample=None, tree_method=None,\n",
       "                                    validate_parameters=False, verbosity=None),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'max_depth': [10, 20], 'n_estimators': [10, 20]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# param_grid = {'n_estimators'    :[100, 200, 300, 400, 500, 600, 1000][-3:],\\\n",
    "#          'max_depth'       :[5, 10, 20, 30, 40, 50][-3:],} \n",
    "param_grid = {\n",
    "    'max_depth': [10, 20],\n",
    "    'n_estimators': [10, 20],\n",
    "#     'max_depth': [10, 15, 20, 30],\n",
    "#     'n_estimators': [10, 15, 25, 50],\n",
    "#     'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#     'reg_alpha': [1.2, 1.3, 1.4, 1.5],\n",
    "#     'reg_lambda': [1, 1.1, 1.2, 1.3, 1.5],\n",
    "#     'subsample': [.5, .6, .7, .8, .9],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator = xgb.XGBRegressor(), param_grid = param_grid, n_jobs=-1, cv=3, verbose=2)\n",
    "\n",
    "grid.fit(train_examples, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=10,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=10, n_jobs=0, num_parallel_tree=1,\n",
       "             objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "             validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.282859387485586 7.765312578596579 20.64977738667102 329.0832214355469\n",
      "78.5389404296875 215.7413330078125 470.17071533203125 4469.127099609367\n"
     ]
    }
   ],
   "source": [
    "pred    = grid.best_estimator_.predict(test_examples)\n",
    "error   = pred - test_targets\n",
    "abs_err = np.abs(error)\n",
    "rel_err = 100. * abs_err / test_targets\n",
    "print(np.percentile(rel_err, 25), np.percentile(rel_err, 50), np.percentile(rel_err, 75), np.percentile(rel_err, 95))\n",
    "print(np.percentile(abs_err, 25), np.percentile(abs_err, 50), np.percentile(abs_err, 75), np.percentile(abs_err, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.484e+03, 2.800e+01, 8.000e+00, 6.000e+00, 2.000e+00, 1.000e+00,\n",
       "        4.000e+00, 1.000e+00, 0.000e+00, 1.000e+00]),\n",
       " array([1.64345298e-03, 3.23743688e+03, 6.47487211e+03, 9.71230734e+03,\n",
       "        1.29497426e+04, 1.61871778e+04, 1.94246130e+04, 2.26620483e+04,\n",
       "        2.58994835e+04, 2.91369187e+04, 3.23743540e+04]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.hist(rel_err, bins=30)\n",
    "plt.hist(rel_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                42,               2540,               2711,\n",
       "                     2868,               2128,              10581,\n",
       "                     2211,              10277,                 32,\n",
       "                       78,                 49,                 50,\n",
       "        'bgsys-gnu-linux',                  6, 'WALLTIME_SECONDS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feat_idx = np.argsort(grid.best_estimator_.feature_importances_)[-15:]\n",
    "features.columns[top_feat_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(pred)):\n",
    "#     pred[i] = min(pred[i], test_examples.WALLTIME_SECONDS.to_list()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = df_filtered.NODES_REQUESTED.tail(int(len(df_filtered.index) * (1-train_size)))\n",
    "cushion = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     8535.000000\n",
       "mean      1940.276975\n",
       "std       2612.474891\n",
       "min     -27416.664551\n",
       "25%       1759.191040\n",
       "50%       1930.456055\n",
       "75%       2169.241211\n",
       "max      46232.265625\n",
       "Name: RUNTIME_SECONDS, dtype: float64"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of difference between runtime and new walltime w/ cushion\n",
    "((pred + cushion) - test_targets).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628\n"
     ]
    }
   ],
   "source": [
    "# Number of jobs cutoff w/ new walltimes\n",
    "array = ((pred + cushion) - test_targets)\n",
    "print(len([item for item in array if item <= 0]))\n",
    "# [item for item in array if item <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i in range(len(array.values)) if array.values[i] <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = test_examples.iloc[indices]\n",
    "examples = examples.drop(examples.columns[(examples == 0).all()], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "walltimes = test_targets.iloc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered.loc[4955]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.357937902753369"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of jobs cutoff\n",
    "len([item for item in ((pred + cushion) - test_targets) if item <= 0])/len(test_targets) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-481.4504645453559"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Walltime hours cutoff\n",
    "sum([item for item in (((pred + cushion) - test_targets)) if item <= 0]) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-392864.5704166667"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nodehours cutoff\n",
    "sum([item for item in (((pred + cushion) - test_targets) * nodes) if item <= 0]) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212688.990390625"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nodehours saved\n",
    "((test_examples.WALLTIME_SECONDS - (pred + cushion)) * nodes).sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1352810.6666666667"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total nodehours\n",
    "(test_examples.WALLTIME_SECONDS * nodes).sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15722007198146354"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saved nodehours percentage\n",
    "((test_examples.WALLTIME_SECONDS - (pred + cushion)) * nodes).sum() / (test_examples.WALLTIME_SECONDS * nodes).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-481.4504645453559"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total walltime cutoff\n",
    "sum([item for item in (((pred + cushion) - test_targets)) if item <= 0]) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2619.4561162651908"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total walltime saved\n",
    "((test_examples.WALLTIME_SECONDS - (pred + cushion))).sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16952.933333333334"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total walltime hours scheduled\n",
    "(test_examples.WALLTIME_SECONDS).sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019363065215323486"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saved walltime percentage\n",
    "((test_examples.WALLTIME_SECONDS - (pred + cushion))).sum() / (test_examples.WALLTIME_SECONDS * nodes).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     8535.000000\n",
       "mean      1104.867255\n",
       "std       3891.087034\n",
       "min      -2243.468262\n",
       "25%      -1429.233887\n",
       "50%       -792.075684\n",
       "75%       2113.581055\n",
       "max      39987.734375\n",
       "Name: WALLTIME_SECONDS, dtype: float64"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of difference between old walltime and new walltime w/ cushion\n",
    "((test_examples.WALLTIME_SECONDS - (pred + cushion))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8.560000e+02\n",
       "mean     8.944864e+05\n",
       "std      4.219037e+06\n",
       "min     -7.880112e+06\n",
       "25%     -7.825122e+05\n",
       "50%     -4.112318e+05\n",
       "75%      1.328536e+06\n",
       "max      4.891375e+07\n",
       "dtype: float64"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of difference between old walltime and new walltime w/ cushion in nodehours\n",
    "((test_examples.WALLTIME_SECONDS - (pred + cushion)) * nodes).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
